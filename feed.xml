<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://healthcare.ai/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.3.1">Jekyll</generator><link href="http://healthcare.ai/feed.xml" rel="self" type="application/atom+xml" /><link href="http://healthcare.ai/" rel="alternate" type="text/html" /><updated>2017-01-11T21:39:58-07:00</updated><id>http://healthcare.ai//</id><title type="html">healthcare.ai</title><entry><title type="html">Know your business question: A focus on readmissions</title><link href="http://healthcare.ai/blog/2017/01/11/know-your-business-question-a-focus-on-readmissions/" rel="alternate" type="text/html" title="Know your business question: A focus on readmissions" /><published>2017-01-11T21:00:00-07:00</published><updated>2017-01-11T21:00:00-07:00</updated><id>http://healthcare.ai/blog/2017/01/11/know-your-business-question-a-focus-on-readmissions</id><content type="html" xml:base="http://healthcare.ai/blog/2017/01/11/know-your-business-question-a-focus-on-readmissions/">&lt;p&gt;As time goes on, we will not only discuss healthcare machine learning (ML) and health in the US at a high level, but also specific ways ML might help drive outcomes improvements. Many health systems are working on reducing their readmission rate—which is often considered a measure of quality of care and can be tied to penalties. For  hospital systems progressing toward ML for readmissions—or any measure—the first step is to identify your most important business questions; the next step is creating a suitable dataset to create the model. There are often several points where business logic dictates decisions related to the dataset and whether one or multiple models are needed to help with a specific process. That’s certainly the case when creating readmission risk models.&lt;/p&gt;

&lt;p&gt;Readmission risk models improve patient quality of life and decrease mortality by providing extra care or surveillance to high-risk patients. Implementing a readmission risk model could serve two different purposes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Identifying high-risk &lt;em&gt;observational&lt;/em&gt; and &lt;em&gt;inpatient&lt;/em&gt; patients as soon after their admission as possible to answer this question: &lt;em&gt;Which in-hospital patients are most at risk for readmission?&lt;/em&gt; By answering this question, doctors, nurses, and in-hospital staff can intervene to try to lower a patient’s readmission risk.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Identifying high-risk &lt;em&gt;discharged&lt;/em&gt; patients as soon after their discharge as possible to answer this question: &lt;em&gt;Which discharged patients are most at risk of readmission?&lt;/em&gt; By answering this question, hospital support staff and transitional services can intervene to try to lower a patient’s readmission risk.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, you might think, “Those are the same use case: they both predict readmissions, even sometimes for the same patients.” But remember, an ML model is only valuable if it provides actionable insight. Nurses are in the optimal position to help when the patient is in the hospital. Furthermore, as we &lt;a href=&quot;http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/&quot;&gt;discussed at length&lt;/a&gt; last week, the data sources for these two use cases are very different. The type and amount of information available at discharge is different from what is available at admission. The best ML models are built custom to a specific dataset and use case. One risk model is not sufficient to answer both questions, and accuracy would almost certainly be compromised if they were naively combined. Though similar, these questions need models that relate to different types of patients and are targeted toward different interventions. Under the hood of the models, we need to use different datasets (to avoid [data leakage])(http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/)—and maybe even different algorithms. The best ML model will always be built to answer a specific question, tailored to specific data, and targeted toward the most effective intervention.&lt;/p&gt;

&lt;p&gt;OK, so we know we need two models for readmission risk modeling. But to complicate things even more, we must keep in mind that the layman definition of readmission and the &lt;a href=&quot;https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/PhysicianFeedbackProgram/Downloads/2014-ACR-MIF.pdf&quot;&gt;CMS definition of readmission&lt;/a&gt; are also subtly different. We must base our outcome variable on a definition very similar to that of CMS because it is consistent with the way many hospital systems track and report outcomes, and the CMS definition of readmissions is ultimately the measure hospital systems are trying to affect. Knowing that definition is critical to building a model to address it.&lt;/p&gt;

&lt;p&gt;Per the CMS definition, patients in the hospital can be &lt;em&gt;emergency department&lt;/em&gt;, &lt;em&gt;observational&lt;/em&gt;, or &lt;em&gt;inpatient&lt;/em&gt;. To be considered an unplanned readmission patients must initially be discharged from the inpatient setting. A patient that is discharged from the emergency department or observation setting cannot be readmitted (0% probability) because they did not meet the inpatient requirement pertaining to the inpatient index admission. Similarly, even after a patient is admitted to the inpatient setting, we still do not yet know their what their discharge disposition or discharge diagnosis will be. If the patient leaves against medical advice or is assigned a cancer-related discharge, for instance, they meet a different set of exclusion criteria and cannot be readmitted (again, 0% probability). While the specific criteria behind the definition of a readmission makes practical sense, it creates a couple of challenges to training, testing, and deploying a readmission risk model that is to be leveraged while patients are still in the hospital.&lt;/p&gt;

&lt;p&gt;At the end of the day, we need to develop the model using the same data that we want it to perform well on in production. For the &lt;em&gt;in-hospital&lt;/em&gt; use case, &lt;em&gt;observational&lt;/em&gt; and &lt;em&gt;emergency department&lt;/em&gt; patients that would ultimately be excluded at discharge must also be excluded from model development. These patients may skew the model toward predicting 0% readmission probability since they are guaranteed to be 0% risk as defined by CMS. This puts us in a tight spot. When &lt;a href=&quot;http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/&quot;&gt;evaluating the model&lt;/a&gt;, it may appear to have higher accuracy by skewing toward low probabilities because it was improperly trained on data that should have been excluded. For the post-discharge use case, the discharge type is available and the model can match the CMS definition more closely. This will likely lead to increased accuracy overall, as more use-case specific data is available.&lt;/p&gt;

&lt;p&gt;From our experience, understanding the definition of the readmission outcome variable, the specific use case, and the timing/target is crucial. There is clearly a trade-off between timeliness and accuracy, and to have the greatest impact on patient outcomes, it is important to develop readmission risk models based on data that reflects the use case in production. Again, the best ML model should answer a specific question, using specific data, with an actionable result. Keep these things in mind and your models will improve.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Taylor Larsen</name></author><category term="overview" /><summary type="html">This post describes the importance of understanding the business questions, use cases, and data when creating a readmission risk model</summary></entry><entry><title type="html">Which regions of the US are healthy?</title><link href="http://healthcare.ai/blog/2017/01/08/us-health-by-county/" rel="alternate" type="text/html" title="Which regions of the US are healthy?" /><published>2017-01-08T16:00:00-07:00</published><updated>2017-01-08T16:00:00-07:00</updated><id>http://healthcare.ai/blog/2017/01/08/us-health-by-county</id><content type="html" xml:base="http://healthcare.ai/blog/2017/01/08/us-health-by-county/">&lt;p&gt;While our previous posts have focused on healthcare machine learning, we’re also excited to post analyses of health data using R and Python. We do this to hopefully elevate the national discussion around health data, enhance the community’s understanding of health in the United States (US), and provide guidance as to how communities and health systems might increase the quality and length of people’s lives. Health Catalyst is an outcomes improvement company, and we realize that the inpatient setting is only one of several venues that affect a person’s health trajectory. Understanding the big picture of health is another way to approach outcomes improvements. These posts will not only attempt to educate on findings about health, but also on how to use R/Python for health data analysis, so we’ll always post links to the &lt;a href=&quot;https://gist.github.com/levithatcher/070496ca48c165d7ced37e0ffcd24dc7&quot;&gt;relevant code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Those who have been in healthcare for any significant amount of time have heard that social determinants of health (SDOH) are important to healthcare outcomes. While it’s hard to overstate the importance of these factors, they’re often not well understood and overshadowed by inpatient optimizations when discussing outcomes improvement. This post is the first in a series where we’ll attempt to untangle the drivers behind population health differences across the US. Today we’ll talk about where the US stands from region to region in terms of social determinants of health. In subsequent posts we’ll discuss whether these differences mostly related to income, air pollution, access to healthy food, long commutes, access to healthcare, opioid addiction, or alcohol abuse.&lt;/p&gt;

&lt;p&gt;To try and answer that we’ll &lt;a href=&quot;https://gist.github.com/levithatcher/070496ca48c165d7ced37e0ffcd24dc7&quot;&gt;use R&lt;/a&gt; and &lt;a href=&quot;http://www.countyhealthrankings.org/sites/default/files/2015%20CHR%20Analytic%20Data.csv&quot;&gt;data&lt;/a&gt; from &lt;a href=&quot;http://www.countyhealthrankings.org/&quot;&gt;countyhealthrankings.org&lt;/a&gt;, which is a fantastic resource on SDOH comparisons by county. We’ll start by presenting a choropleth map of median household 2015 &lt;a href=&quot;http://www.countyhealthrankings.org/measure/median-household-income&quot;&gt;income by county&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/MedianIncomeByCounty.jpg&quot; alt=&quot;IncomeByCounty&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, what we find is that there are large regional differences in household income. Broadly, the Northeast, the West Coast, and metropolitan areas are associated with higher personal incomes, compared with rural areas and the South. Note that occasionally there is high intra-state variation, such as in Texas, compared to the more uniform median incomes across counties in Minnesota. But how do these regional variations in incomes correspond with healthcare outcomes? Data on &lt;a href=&quot;http://www.countyhealthrankings.org/measure/low-birthweight&quot;&gt;Low Birth-Weight&lt;/a&gt; (LBW), i.e., live births under ~5 lbs 8 oz (2500 g), provides a &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/7633862&quot;&gt;helpful link&lt;/a&gt; between social determinants and actual healthcare outcomes, as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“&lt;a href=&quot;http://www.countyhealthrankings.org/measure/low-birthweight&quot;&gt;LBW indicates maternal exposure to health risks&lt;/a&gt; in all categories of health factors, including her health behaviors, access to health care the social and economic environment she inhabits, and environmental risks to which she is exposed. In terms of the infant’s health outcomes, LBW serves as a predictor of premature mortality and/or morbidity over the life course and for potential cognitive development problems.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Pulling data from &lt;a href=&quot;http://www.countyhealthrankings.org/measure/low-birthweight&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://gist.github.com/levithatcher/070496ca48c165d7ced37e0ffcd24dc7&quot;&gt;processing with R&lt;/a&gt;, we plot the percentage of county live births with birth-weight under 5 lbs 8 oz:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/LowBirthWeightByCounty.jpg&quot; alt=&quot;LBWByCounty&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While there’s a lot that could be unpacked here, we’ll simply note that the same regions that had lower personal incomes also have a higher percentage of LBW. Not a huge surprise—it is surprising, however, how much intra-state variation is present (like in NV and CO) and how the Deep South has rates of LBW that are often twice that of Minnesota and Wisconsin.&lt;/p&gt;

&lt;p&gt;While income appears to be associated with new-born morbidity and mortality, how does it affect populations later in life? We use &lt;a href=&quot;http://www.countyhealthrankings.org/measure/premature-death-ypll&quot;&gt;premature mortality&lt;/a&gt; &lt;a href=&quot;http://www.countyhealthrankings.org/sites/default/files/2015%20CHR%20Analytic%20Data.csv&quot;&gt;data&lt;/a&gt; from &lt;a href=&quot;http://www.countyhealthrankings.org/&quot;&gt;countyhealthrankings.org&lt;/a&gt;, where a premature death is defined as occurring before 70 years of age, to answer this question. For each county, per 100k people, the years of death before 75 are summed. Think of it as incidence of early death, per county:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/PrematureDeathByCounty.jpg&quot; alt=&quot;PrematureDeathByCounty&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Compared to LBW, it appears that premature mortality more closely corresponds with median county income. Note how the high incidence of premature mortality across Arkansas, Tennessee, and Kentucky closely tracks income (comparing with the first figure). Broadly, Appalachia appears to suffer more from deaths of prime-age adults compared to LBW (while the South appears to suffer greatly from both). Note that while the rust belt (i.e., PA, OH, IN, MI, IL, and WI) certainly has other issues, they seem to be doing a good job of keeping prime-age people alive, especially compared to Appalachia and the Deep South.&lt;/p&gt;

&lt;p&gt;While it’s old-hat to say that population health in the Deep South isn’t fantastic, let’s go one step further and see which counties in the US punch above their weight when it comes to using resources effectively. In other words, which counties are doing well for how poor they are. This is the metric that will let us understand what it is that poor and middle-income counties do well in terms of keeping people healthy.&lt;/p&gt;

&lt;p&gt;To get at this, we create percentiles for each county in terms of LBW (where 100 is best) and then subtract (for each county) the percentiles for income. We can call this the county Punch-Above-Their-Weight Index or PATWI for short. Before plotting the entire country, let’s look at LBW PATWI for the top ten counties in terms of income:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/TableHighestIncomeAndLBW.png&quot; alt=&quot;TableHighestIncomeandLBW&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, in this and the following tables, the PATWI column is just the fifth column minus the fourth column. While the counties above do have good scores in terms of LBW, it’s difficult for rich counties to punch above their weight, since they’re the 800-lb gorillas. We do, however, see the benefit of the PATWI, since the richest counties in the US &lt;em&gt;aren’t&lt;/em&gt; those with the best LBW scores.&lt;/p&gt;

&lt;p&gt;Which counties have the best LBW, considering their income?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/TableHighestLBWPATWIAndIncome.png&quot; alt=&quot;TableHighestPATWIAndIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s impressive that these counties, which have median incomes one-third of that of Loudoun County (VA), have better LBW scores than Loudoun County. Note that the best LBW PATWI scores come from a scattered grouping of states, although, Michigan (MI) impressively has three entries and Missouri (MO) has two. That’s definitely good news for MI and MO public health officials. Note that this PATWI measure doesn’t just bias towards the poorest counties, either. While Buffalo County, SD—perhaps the poorest county in the nation—has a high LDW PATWI score, none of the counties from the Deep South or Appalachia show up in this list. Here’s LDW PATWI score for all counties:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/LBWComparedToIncomeByCounty.jpg&quot; alt=&quot;LBWComparedToIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that positive values here denote that the county is doing better at LBW than expected, based on their income*. Amongst other findings, we see that even though northern half of West Virginia is quite poor, they’re doing better than expected at helping mothers carry and deliver healthy babies. The Deep South, however, is doing about as poorly (or worse) than one would expect by looking at their income along. Note that Missouri is doing better than expected, as is rural Oregon, Minneapolis, and Wisconsin. Recall that metro areas tend to be richer than rural areas, so it’s impossible for richest areas like the Bay Area and NYC tend to have a high score (since their percentiles can’t go over 100)—this limitation makes this plot more of a measure of how middle and low income counties are doing relative to their income.
Let’s do the same for the premature mortality metric we discussed above—in other words, which counties are doing a great job of avoiding early mortality. We’ll start with the richest counties:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/TableHighestIncomeAndPrematureDeath.png&quot; alt=&quot;TableHighestIncomeAndLBW&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yup, the richest counties are great at keeping people alive—even more so than avoiding low birth-weight (compare with the first table above). But, which poor or middle income counties are best at punching above their weight when it comes to keeping their citizens alive?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/TableHighestPrematureDeathPATWIAndIncome.png&quot; alt=&quot;TableHighestLBWPATWIAndIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s impressive how these relatively poor counties are achieving the top 10-20th percentile in terms of avoiding premature deaths (note that we’re using higher percentiles to mean good outcomes). It is an interesting mix of counties, indeed. Santa Cruz (AZ), Presidio County (TX), and Maverick County (TX) border Mexico; Crowley County (CO) has the largest per-capita prison population in the country. Madison County, home to BYU-Idaho, is &lt;a href=&quot;http://www.slate.com/articles/life/map_of_the_week/2012/02/mormon_population_in_the_u_s_an_interactive_map.html&quot;&gt;~80% Mormon&lt;/a&gt;, which likely means that county overall has low rates of alcohol, tobacco, and drug dependency. In a future post we’ll go into how certain counties are punching above their weight, and for now offer the national view of premature-death PATWI score:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/Post10CountyHealthOverview/PrematureDeathComparedToIncomeByCounty.jpg&quot; alt=&quot;PrematureDeathComparedToIncome&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall that darker means that the county is doing well. At this level, what we notice is that Wisconsin, Michigan, and Missouri are doing a good job keeping prime-age people alive compared to Nevada and Wyoming. Knowing why this is occurring can significantly change how a health system interacts with its patients.&lt;/p&gt;

&lt;p&gt;At Health Catalyst, we strive to understand the significant way social determinants of health can affect decision making at health systems from the Deep South to the Bay Area. Our clients are located all across the continent. High-value improvements in one health system are not necessarily the best everywhere, and we’d be cheating ourselves to only look for improvements at inpatient units. We’re committed to leveraging the tools of population health to improve patient outcomes across the board. 
This post is the first of many on population health, and in a future post we’ll dig more into which social determinants are most driving regional variations of LBW and premature death.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;

&lt;p&gt;* The distribution of county median income has a long tail, which means that when subtracting county income percentiles from the corresponding LBW or premature death percentiles, the result is typically positive.&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">While our previous posts have focused on healthcare machine learning, we’re also excited to post analyses of health data using R and Python. We do this to hopefully elevate the national discussion around health data, enhance the community’s understanding of health in the United States (US), and provide guidance as to how communities and health systems might increase the quality and length of people’s lives. Health Catalyst is an outcomes improvement company, and we realize that the inpatient setting is only one of several venues that affect a person’s health trajectory. Understanding the big picture of health is another way to approach outcomes improvements. These posts will not only attempt to educate on findings about health, but also on how to use R/Python for health data analysis, so we’ll always post links to the relevant code.</summary></entry><entry><title type="html">Data leakage in healthcare machine learning</title><link href="http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/" rel="alternate" type="text/html" title="Data leakage in healthcare machine learning" /><published>2017-01-06T15:00:00-07:00</published><updated>2017-01-06T15:00:00-07:00</updated><id>http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning</id><content type="html" xml:base="http://healthcare.ai/blog/2017/01/06/data-leakage-in-healthcare-machine-learning/">&lt;p&gt;Note: this is a technical post.&lt;/p&gt;

&lt;p&gt;To leverage lessons learned during our model building engagements here at Health Catalyst, let’s explore the &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.365.7769&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;subject of data leakage&lt;/a&gt;. Data leakage occurs when a predictive model is trained using information that is available in training data but not actually available for predicting outcomes in production. Models with data leakage tend to be very accurate in development, but perform poorly in production, where they are ultimately used.&lt;/p&gt;

&lt;p&gt;More specifically, leakage in the context of healthcare machine learning occurs when:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A feature is used to train the model that would &lt;em&gt;not be available in production at the time of prediction&lt;/em&gt;. An example of this might be using the number of oral medications a patient is currently taking to predict length of stay at admission when medication reconciliation may not take place for up to 24 hours following admission.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A feature is used to train the model that would &lt;em&gt;not be available in production prior to the outcome variable being populated&lt;/em&gt;. An example of this might be using a response from a quality of life phone survey to predict readmissions when the survey is not administered until three months after the discharge. At that point, it would already be known whether the patient had been readmitted or not. When training a model, the algorithm has no idea whether a feature was populated prior to the target variable in the same row.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A feature is used to train the model that is &lt;em&gt;outside the scope of the model’s intended use case&lt;/em&gt;. An example of this might exist when trying to predict the probability of a patient having heart failure and using the hospital unit associated with the patient without considering that the hospital unit may be disease or service specific (like a cardiac unit).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;correct outcome is leaked into the test data&lt;/em&gt; through a variable that inherently proxies for the outcome. An example of this might be predicting the probability that a patient will pay their balance due on time and using a variable that indicates whether the patient has been contacted by the hospital accounts receivable department which only takes place when a patient is late on payment.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;What can happen?&lt;/em&gt; Leakage can lead to poor generalization, overfitting, and over-estimation of a model’s performance. The ultimate negative impact of leakage is the deployment of a less useful model than if no leakage was present. Considering the examples described above, leakage can result in the inclusion of a variable that appears predictive during training, but due to missing data and/or imputation in production, the variable is either not predictive, is skewed in power, or only appropriate in certain use cases. Leakage will often raise the accuracy of a model in training, but make predictions that can’t be trusted in deployment.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;How does one prevent it?&lt;/em&gt; Through proactive analysis of potentially predictive variables and direct involvement of subject matter experts (SMEs) during variable selection, leakage can most likely be avoided. It is important to profile each variable to determine when it was generated and how its values are distributed when comparing training data to production data. Involving clinical, operational, and data SMEs will reduce the likelihood of leakage by improving understanding of nuances in the data. It is also important to understand when the prediction needs to take place so that predictive variables can be sourced accordingly; and the timing of the prediction should be based on the use case for the predictive model output. In summary, one must scrutinize the data they are using and keep the business question in mind when building the model.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Does an existing model have data leakage?&lt;/em&gt; As described above, profiling each variable and consulting with SMEs may help to identify more obvious leakage. Another way of identifying leakage is to compare the model’s actual performance in production to the model’s performance observed during training and testing. This can expose important discrepancies that might be the result of leakage and in depth analysis comparing training data to production data may be required.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;How does one fix an existing model?&lt;/em&gt; If leakage is identified after a model has been developed and/or deployed, it is important to remediate the issue. Redefining a variable and retraining the model can eliminate leakage and allow the variable to remain in the model. Another solution for eliminating leakage is to remove the variable and retrain the model, while exploring other leakage free variables and proxies that could be added. Though perceived model performance might take a hit when data leakage is avoided/eliminated, the predictions on new data in production will be more accurate and useful.&lt;/p&gt;

&lt;p&gt;Clearly, leakage is an issue that we face on a regular basis, especially when dealing with all the complexities associated with healthcare data. Through experience and diligent analysis, it is an issue we can easily avoid. &lt;a href=&quot;http://healthcare.ai/&quot;&gt;Healthcare.ai&lt;/a&gt; has several tools to help understand the timing, scope, and source of variables when model building, which can be used to eliminate data leakage during the model’s initial creation. Taking these steps will mean deploying useful models that are widely adopted, and ultimately improve healthcare outcomes.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Taylor Larsen</name></author><category term="overview" /><summary type="html">This blog will describe data leakage along with its causes, impacts, and fixes in the context of healthcare machine learning</summary></entry><entry><title type="html">Applications of healthcare machine learning</title><link href="http://healthcare.ai/blog/2016/12/22/applications-of-healthcare-machine-learning/" rel="alternate" type="text/html" title="Applications of healthcare machine learning" /><published>2016-12-22T21:39:11-07:00</published><updated>2016-12-22T21:39:11-07:00</updated><id>http://healthcare.ai/blog/2016/12/22/applications-of-healthcare-machine-learning</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/22/applications-of-healthcare-machine-learning/">&lt;p&gt;Now that we have been through some of the applications of machine learning (ML) in mainstream technology, we thought it would be nice to give a broader overview of some of the different types of ML and how they might be applied to improve patient care. &lt;a href=&quot;http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/&quot;&gt;We explored the algorithms&lt;/a&gt; that currently make up healthcare.ai, and alluded to the fact that there is lots of room for expansion. We’ll take this post as an opportunity to speculate on where healthcare ML could go in the near and distant future. Along the way, we’ll discuss the different types of ML algorithms and give examples of their use in healthcare. 
At its most basic definition, machine learning refers to a group of algorithms that learn from data. These algorithms are different from &lt;a href=&quot;https://fiftyexamples.readthedocs.io/en/latest/celsius.html&quot;&gt;conventional ones&lt;/a&gt; since they work using examples rather than rules. If you went to the hospital for flu like symptoms, a doctor thinking along the lines of traditional algorithms might say, “You have a fever, aches, general weakness, and no cold symptoms. This looks like the flu.” A different doctor, thinking like an ML algorithm, would say, “Hmmm, your symptoms are the same as 50 recent patients who had the flu. You probably do too.” Silly example, but it’s worth noting a couple of things. First, the ML doctor doesn’t actually need to know anything about the flu before they start making diagnoses. Second, their diagnoses probably won’t be very good until they have seen a lot of examples to compare against.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Classification vs. Regression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The above scenario is an example of a classification machine learning problem. A classification algorithm will give a probability score of a person having the disease, or, more broadly, the probability of event happening vs. not. Healthcare.ai has already implemented some of the simplest &lt;a href=&quot;http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/&quot;&gt;algorithms&lt;/a&gt; to answer questions like:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What is the likelihood that a patient will develop a central line infection?&lt;/li&gt;
  &lt;li&gt;What is the likelihood that a COPD patient will be readmitted within 90 days of discharge?&lt;/li&gt;
  &lt;li&gt;What is the likelihood that a person will no-show for their appointment?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These questions are posed with a lot of example data and the expectation that a model will give a probability from 0 (low) to 1 (high). It’s up to us to draw the line of what we call a positive prediction and what we call a negative prediction. On the other hand, a regression algorithm will predict a continuous value. Here are some examples of questions that we have or plan to tackle using regression algorithms:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How many days will a patient need to stay in the hospital?&lt;/li&gt;
  &lt;li&gt;How many people do we need on staff in the ED on a given night?&lt;/li&gt;
  &lt;li&gt;How much money will a patient cost the health system over the next year?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are a lot of exciting questions that can be answered with very basic machine learning! As we’ve said before, we are focused on trying to answer the questions that will make the most impact right now. Luckily for us, there is still a lot of low-hanging fruit in healthcare for our team to address. As long as there is good example data, ML could help answer a huge range of questions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Supervised vs. Unsupervised&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All the problems that this post has discussed so far are supervised machine learning problems. For each of these there is a ground truth associated with every patient example being used to train the model. The other major type of machine learning is unsupervised. There is no ground truth associated with the data. The algorithms in this category are largely related to identifying patterns and similarity, and using them to group or stratify data into different categories. This type of functionality is a high priority capability that we are working on implementing in healthcare.ai. In the very near future, we hope to be able to use clustering methods and anomaly detection to answer questions like:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Does this patient (who hasn’t been associated with diabetes) belong in a diabetes registry? Or a heart disease registry? And with what likelihood?&lt;/li&gt;
  &lt;li&gt;How similar are my high-utilizing patients? Do they fall into particular clusters? What can we learn about the characteristics of these separate clusters?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These questions are typically more nebulous than supervised learning problems, but useful insights can still be gathered. For example, there would be value in labeling a non-diabetic patient as a person to watch and intervene before they ever develop diabetes. This is great information to have and ML will absolutely make an impact by answering such questions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Future&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The third arm of machine learning that has especially gotten a lot of attention lately is in Artificial Intelligence (AI), mostly implemented with what’s called deep learning. &lt;a href=&quot;https://www.tesla.com/autopilot&quot;&gt;Self-driving cars&lt;/a&gt;, &lt;a href=&quot;https://www.facebook.com/notes/mark-zuckerberg/building-jarvis/10154361492931634/&quot;&gt;home assistants&lt;/a&gt;, and &lt;a href=&quot;http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?smid=pl-share&amp;amp;_r=0&quot;&gt;translation services&lt;/a&gt; are some applications of AI. This is the cutting edge in ML right now. In the 2-5 year timeframe, we will start to see more mainstream impact from these techniques. In healthcare, the first big use case is for image analysis in radiology and pathology departments. It’s possible that computers will learn to assess images with high speed and accuracy in the very near future. When the community is ready for adoption, we will be excited to provide these tools in healthcare.ai.
Hopefully this post helped you to understand the different types of machine learning and begin to think about the types of questions that can be reliably answered. There will never be any shortage of work for machine learning. The bottleneck is the number of people with the necessary expertise. As we’ve said before, another goal of healthcare.ai is to help commoditize machine learning in healthcare. If you’d like to get involved, please &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r&quot;&gt;start using and contributing to the package&lt;/a&gt; on your data and feel free to &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with questions!&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="overview" /><summary type="html">Now that we have been through some of the applications of machine learning (ML) in mainstream technology, we thought it would be nice to give a broader overview of some of the different types of ML and how they might be applied to improve patient care. We explored the algorithms that currently make up healthcare.ai, and alluded to the fact that there is lots of room for expansion. We’ll take this post as an opportunity to speculate on where healthcare ML could go in the near and distant future. Along the way, we’ll discuss the different types of ML algorithms and give examples of their use in healthcare. 
At its most basic definition, machine learning refers to a group of algorithms that learn from data. These algorithms are different from conventional ones since they work using examples rather than rules. If you went to the hospital for flu like symptoms, a doctor thinking along the lines of traditional algorithms might say, “You have a fever, aches, general weakness, and no cold symptoms. This looks like the flu.” A different doctor, thinking like an ML algorithm, would say, “Hmmm, your symptoms are the same as 50 recent patients who had the flu. You probably do too.” Silly example, but it’s worth noting a couple of things. First, the ML doctor doesn’t actually need to know anything about the flu before they start making diagnoses. Second, their diagnoses probably won’t be very good until they have seen a lot of examples to compare against.</summary></entry><entry><title type="html">Which algorithms are in healthcare.ai?</title><link href="http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/" rel="alternate" type="text/html" title="Which algorithms are in healthcare.ai?" /><published>2016-12-21T20:17:11-07:00</published><updated>2016-12-21T20:17:11-07:00</updated><id>http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/">&lt;p&gt;Machine learning has been around for decades and has been used to solve lots of problems. Some of these include &lt;a href=&quot;http://ats.cs.ut.ee/u/kt/hw/spam/spam.pdf&quot;&gt;spam filtering&lt;/a&gt; for email, &lt;a href=&quot;http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html&quot;&gt;suggestions on Netflix&lt;/a&gt;, &lt;a href=&quot;http://qz.com/571007/the-magic-that-makes-spotifys-discover-weekly-playlists-so-damn-good/&quot;&gt;optimized playlists on Spotify&lt;/a&gt;, &lt;a href=&quot;https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf&quot;&gt;custom recommendations on Amazon&lt;/a&gt;, &lt;a href=&quot;https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78#.7b9c9jmg7&quot;&gt;facial recognition on Facebook&lt;/a&gt;, &lt;a href=&quot;https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html&quot;&gt;voice recognition&lt;/a&gt; on your phone, &lt;a href=&quot;http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html&quot;&gt;language translation&lt;/a&gt; on demand, &lt;a href=&quot;http://fusion.net/story/142326/the-new-google-photos-app-is-disturbingly-good-at-data-mining-your-photos/&quot;&gt;image search&lt;/a&gt; in your photo app, and &lt;a href=&quot;https://www.kaggle.com/wiki/DataScienceUseCases&quot;&gt;many more&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While reading that long and varied list, you may be wondering where healthcare stands by comparison. Even though machine learning can solve many problems in healthcare, the field has not yet seen significant adoption. As &lt;a href=&quot;http://healthcare.ai/blog/2016/12/01/welcome-to-healthcareai/&quot;&gt;we’ve mentioned&lt;/a&gt;, the goal of healthcare.ai is to change that.&lt;/p&gt;

&lt;p&gt;We plan to bring the benefits of machine learning into healthcare by starting with the low-hanging fruit. Health Catalyst is a very practical company and that is reflected in &lt;a href=&quot;http://healthcare.ai/&quot;&gt;healthcare.ai&lt;/a&gt;. While many of the machine learning projects mentioned above are using advanced algorithms like &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;deep learning&lt;/a&gt;, healthcare.ai is instead starting with the workhorses of the algorithm world. Note that this a different approach to healthcare machine learning compared to that of &lt;a href=&quot;https://research.google.com/teams/brain/healthcare/&quot;&gt;Google&lt;/a&gt; and &lt;a href=&quot;http://searchhealthit.techtarget.com/opinion/Microsoft-Project-Adam-may-reach-healthcare-specialties&quot;&gt;Microsoft&lt;/a&gt;, which are focusing on the sexier (but less practical) deep learning applications in healthcare.&lt;/p&gt;

&lt;p&gt;Before starting the discussion on healthcare.ai algorithm choices, I should note that we’ll focus on our R package and on &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_classification&quot;&gt;classification&lt;/a&gt; (rather than regression), since &lt;em&gt;most&lt;/em&gt; problems in healthcare revolve around predicting &lt;a href=&quot;http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created/&quot;&gt;Yes or No&lt;/a&gt; rather than a continuous variable. To make the terminology clear, it should also be stated that a machine learning algorithm, when paired with data, leads to a model. The algorithms exist off the shelf. The great value-add comes from pairing the proper algorithm with the data of interest. Healthcare.ai has open-sourced tools that allow you to easily match your data with suitable algorithms, create models, and help you answer your most important business questions. The models that you and Health Catalyst create are proprietary, but the tools used to make those models are free.&lt;/p&gt;

&lt;p&gt;Arguably the most simple and common algorithm when trying to classify things via machine learning is &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;logistic regression&lt;/a&gt;. (Note that despite the name, this algorithm is used for classification problems.) We love it because it’s easy to use, quick to finish, and easy to interpret. We’ve been using this algorithm, but with a twist to it.&lt;/p&gt;

&lt;p&gt;We built healthcare.ai with the goal of providing users with guidance as to which &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;features&lt;/a&gt; (i.e., variables) were predictive and worth using when building a model. This drove us to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Lasso_(statistics)&quot;&gt;lasso&lt;/a&gt;, which is a linear model much like logistic regression, but it provides feedback on which features should or shouldn’t be included in the model. You might say, “Well, couldn’t logistic regression just ignore features that weren’t predictive?” Yes, but when the user has brought 20-40 variables into a focused dataset—what Health Catalyst calls a source area mart—to see if they help predict Sepsis, they’ll often only want to keep those variables that are predictive, as ETL processes often have hard resource constraints. With knowledge of how important each feature is, one can often remove many non-predictive variables from a model without any significant loss in accuracy.&lt;/p&gt;

&lt;p&gt;Next to linear models like lasso and logistic regression, the most common algorithm in machine learning is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;&gt;random forest&lt;/a&gt;. It’s different in that it can model non-linear relationships accurately. The random forest algorithm is an ensemble method, which aggregates the result of 100+ randomized decision trees to produce a prediction. While it’s a little harder to interpret than linear algorithms (like lasso), it typically doesn’t need much &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;&gt;hyperparameter tuning&lt;/a&gt;, can run quite quickly, and from our experience often provides more accurate models (compared to linear algorithms) for common healthcare questions. These reasons drove us to include a random forest option in healthcare.ai.&lt;/p&gt;

&lt;p&gt;To back up a bit, in typical machine learning problems row order doesn’t matter. If you think of the simple example of housing data in Salt Lake City to determine the relationship between square footage and house price, it doesn’t matter if the row for house 10045 was listed before house 10057 in the dataset. Those two rows are treated independently. In longitudinal datasets however, the relationship between the rows often &lt;em&gt;does&lt;/em&gt; matter. In certain clinical datasets, we often find multiple entries for the same person, showing the person’s progression over time. When this progression is important to the business question that’s being addressed with machine learning, basic machine learning might not be suitable.&lt;/p&gt;

&lt;p&gt;This frequent longitudinal aspect of healthcare data is why we’re excited to offer linear &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixed_model&quot;&gt;mixed models&lt;/a&gt; in our the R version of healthcare.ai. Mixed models offer the ability to combine a personal trend with a population trend. It’s called a mixed model because it combines fixed effects (i.e., those that relate to the population as a whole) with random effects (i.e., those that are innate to that individual). &lt;a href=&quot;http://www.bodowinter.com/tutorial/bw_LME_tutorial.pdf&quot;&gt;This paper&lt;/a&gt; provides a good introduction to the topic. From our experience, this algorithm is slow compared to random forest and lasso, but can create a better model if the prediction at hand significantly depends on a person’s history (i.e., think diabetic amputation risk rather than &lt;a href=&quot;https://en.wikipedia.org/wiki/Central_venous_catheter#Bloodstream_infections&quot;&gt;CLABSI&lt;/a&gt;). As always, healthcare.ai makes it easy to see how this algorithm performs on your dataset, and determine if it does a better job than the more common lasso and random forest algorithms.&lt;/p&gt;

&lt;p&gt;Again, the goal of healthcare.ai is to help the medical community use machine learning to improve healthcare outcomes. On the first pass, we have implemented what we feel are the simplest and most effective algorithms specific to healthcare data. We’ll certainly expand in the future, but for now there are a lot of efficiency gains to be &lt;achieved&gt;&lt;/achieved&gt; with basic algorithms, standardized performance metrics, smart implementation, and centralized documentation.&lt;/p&gt;

&lt;p&gt;If you want more detail, check out &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r&quot;&gt;our code&lt;/a&gt;. If you have questions or feedback, &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;contact us&lt;/a&gt;!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="algorithms" /><summary type="html">Machine learning has been around for decades and has been used to solve lots of problems. Some of these include spam filtering for email, suggestions on Netflix, optimized playlists on Spotify, custom recommendations on Amazon, facial recognition on Facebook, voice recognition on your phone, language translation on demand, image search in your photo app, and many more.</summary></entry><entry><title type="html">Model evaluation using ROC Curves</title><link href="http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/" rel="alternate" type="text/html" title="Model evaluation using ROC Curves" /><published>2016-12-15T15:37:00-07:00</published><updated>2016-12-15T15:37:00-07:00</updated><id>http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/">&lt;p&gt;Before a new technique in healthcare can be introduced to patient use, it must pass a rigorous set of quality standards. Then, to actually be adopted and see widespread use, a technique must be trusted and accepted by physicians and other front line care workers. For example, new drugs are evaluated in several steps before making into human trials, and then still have several hurdles to clear before they can be accepted as standard of care. Machine learning is poised to make a significant impact in clinical care in the near future, but it is not exempt from these same checks and developmental hurdles.&lt;/p&gt;

&lt;p&gt;The main goal of the healthcare.ai is to improve healthcare outcomes. As detailed in a previous post, we provide the tools and models that can use existing data to help intelligently guide clinical decisions. There has to be trust and transparency in these models if they are to make an impact and see long-term adoption. Just like a new drug, every model we build is evaluated to make sure that it’s high-quality before it is pushed into production. We evaluate models to compare them with other techniques, know when in their development they are ready for production, and to get an overall sense of how much we should trust them. Interestingly enough, one common way to do this (at least in classification problems) borrows from other areas of medicine and uses a &lt;em&gt;Receiver Operating Characteristic Curve (ROC).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Before we can get to the curve itself, we need a few definitions. Let’s say we’ve generated a machine learning model to predict the likelihood of 30-day readmission in a set of patients. The model gives a probability (between 0 and 1) for each person of how likely they are to be readmitted. 30 days later, the &lt;em&gt;True Positive Rate (TPR)&lt;/em&gt; is the proportion of actual readmissions that the test correctly predicted would be readmitted. The &lt;em&gt;False Positive Rate (FPR)&lt;/em&gt; is the proportion of patients whom the model predicted would be readmitted, but were not. In order to make these black and white predictions, we must pick a decision boundary somewhere between 0 and 1. Remember, the model gives a probability, not a definitive answer. If we were to choose 0.9, we would say that everyone with readmittance probability above 0.9 is a readmission, everyone below is not. We could then calculate the TPR and FPR, and have a measure of how well our model performed at 0.9 decision boundary. As you might have guessed, the decision boundary is a sticky spot. If we were choose 0.8 to increase the TPR, it will come at the expense of a larger FPR. The three parameters are tied to one another in a way that makes models hard to interpret and discuss.&lt;/p&gt;

&lt;p&gt;The ROC is a common way to avoid this. It is a graphical representation of the balance between TPR and FPR at &lt;em&gt;every&lt;/em&gt; possible decision boundary. The Area Under the Curve (AUC) is that magic solution that we have been looking for. The AUC is a single number that can evaluate a model’s performance, regardless of the chosen decision boundary. The perfect machine learning model will have an AUC of 1.0 (cyan), while a random one will have an AUC of 0.5 (orange). A good model will be over 0.7, a great one will be over 0.85. It might not be possible to perfectly classify a data set, but the AUC is a good way to compare models on that data, across patient cohorts, and give a sense of how trustworthy that model is in general.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/AUCPost_ROCExample.png&quot; alt=&quot;Example ROC Curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Whenever we are getting ready to deploy a model into use, we need to evaluate its overall performance. The AUC gives us a transparent, easy-to-interpret way to do that. Of course, it has limitations. For example, the usefulness of the ROC curve begins to break down with heavily imbalanced classes, obviously a big problem for healthcare data. One solution is to use AUC from a Precision-Recall Curve, but we’ll save that for a future post. If you’re interested in trying out ROC curves on your data, you’ll find some handy tools already built into the healthcare.ai package to help you evalutate your models. Finally, if you’re hungry for more, there are many &lt;a href=&quot;https://classeval.wordpress.com/introduction/introduction-to-the-roc-receiver-operating-characteristics-plot/&quot;&gt;great tutorials online&lt;/a&gt; for ROC curves.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="overview" /><summary type="html">This blog will describe the motivation and uses of the ROC curve</summary></entry><entry><title type="html">What models has Health Catalyst created with healthcare.ai?</title><link href="http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created/" rel="alternate" type="text/html" title="What models has Health Catalyst created with healthcare.ai?" /><published>2016-12-12T09:28:05-07:00</published><updated>2016-12-12T09:28:05-07:00</updated><id>http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created/">&lt;p&gt;After reading a few articles on healthcare.ai, some of you may be saying, well, that’s great–but what has Health Catalyst actually used it for? Since Health Catalyst has been open with sharing the tool set, it only makes sense that they’d also be willing to share details of its use. As the Director of Data Science at Health Catalyst and founder of healthcare.ai, I oversee all client predictive engagements, and will make a point of frequently updating the community on our work. If you have questions, comments, or criticism, please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The goal of healthcare.ai was to provide a simple, flexible tool to streamline healthcare machine learning. This means that it works across financial, operational, and clinical realms. If a health system has a business question that they want predictions for, we will make healthcare.ai flexible enough to cover that use case. Today we’ll briefly cover three recent predictive project, and detail more for a future post.&lt;/p&gt;

&lt;p&gt;Let’s start with finance. Uncompensated care is a growing problem at most health systems. To help a counter this trend, we’ve started creating propensity-to-pay models. Recall that each health system interested in using machine learning is provided a custom model, tailored to their data. In this propensity-to-pay project, for each person with an open account with the health system, each month the probability of payment is calculated. This personal probability can be used to determine 1) who may need reminders, 2) who may need financial assistance, and 3) how the likelihood of payment changes over time and after particular life events. A few &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;features&lt;/a&gt; that were important to this model turned were things like whether the person paid last month (surprise!), account balance, a person’s age, the month of the year, etc. These may vary for your health system (as your demographics are likely a bit different), but healthcare.ai makes it easy to customize the model to &lt;em&gt;your&lt;/em&gt; payment data.&lt;/p&gt;

&lt;p&gt;Those who have ever worked in a clinical setting know how hard it is to maintain schedules that keep both clinicians and patients happy. Slots are often over or under-booked because someone showed up late, didn’t show up, or showed up without warning. Health Catalyst taken a first pass at this problem via a no-show predictive model. In this engagement, we gathered all past data on the characteristics of people that had showed or hadn’t showed for their appointments, and created an accurate predictive model to assess, with each scheduled appointment, the risk of a no-show. If the clinic feels that they can reduce their no-show rate by extra phone calls (or other measures), this predictive guidnaces assures that the resources used are efficiently allocated. If, on the other hand, the clinic has found that it’s quite hard to reduce the no-show rate (even with this guidance), they can use the probability scores to over-book particular slots, such that clinical scheduling is optimized using past data. &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;Features&lt;/a&gt; that were particularly helpful in this prediction were prior number of cancellations, appointment type, and week of the year.&lt;/p&gt;

&lt;p&gt;Finally, we’ll touch on a clinical case. Many health systems are &lt;a href=&quot;http://www.modernhealthcare.com/article/20150803/NEWS/150809981&quot;&gt;penalized&lt;/a&gt; if their 30-day readmissions rate is too high. While general readmissions models are possible via healthcare.ai, we’ve found that focusing on particular disease cohorts (such as for heart failure, sepsis, or COPD) allows us to create a much more accurate model. We’ve had multiple engagements recently related to 30 and 90-day COPD readmissions. How it works is that the relevant data on past patients (and whether they had a readmission or not) is collected into a &lt;a href=&quot;https://www.healthcatalyst.com/late-binding-data-warehouse/late-binding-data-bus/sam-designer/&quot;&gt;subject area mart&lt;/a&gt;, and a couple of different algorithms are used to create models. Once we find the column set and algorithm that together produce the most accurate model, we save the model and integrate it into our nightly &lt;a href=&quot;https://en.wikipedia.org/wiki/Extract,_transform,_load&quot;&gt;ETL&lt;/a&gt;. This way, clinicians  receive daily guidance as to which of their patients is most likely to be readmitted. Among others, &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;features&lt;/a&gt; like prior readmissions, pre-existing conditions, and specific facility were particularly helpful to this model.&lt;/p&gt;

&lt;p&gt;Considering the resource constraints present in many hospital units, this type of machine learning guidance can be crucial to efficiently deploying resources toward achieving business goals (i.e., reducing readmissions, reducing 1-yr mortality, preventing &lt;a href=&quot;https://www.cdc.gov/hai/&quot;&gt;HAIs&lt;/a&gt;, etc). As time goes on, we’ll detail more of these predictive projects, and explain how they might be useful to your health system.&lt;/p&gt;

&lt;p&gt;Thanks, and please &lt;a href=&quot;http://healthcare.ai/contact.html&quot;&gt;reach out&lt;/a&gt; with any questions!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">After reading a few articles on healthcare.ai, some of you may be saying, well, that’s great–but what has Health Catalyst actually used it for? Since Health Catalyst has been open with sharing the tool set, it only makes sense that they’d also be willing to share details of its use. As the Director of Data Science at Health Catalyst and founder of healthcare.ai, I oversee all client predictive engagements, and will make a point of frequently updating the community on our work. If you have questions, comments, or criticism, please reach out.</summary></entry><entry><title type="html">The technical need for healthcare.ai</title><link href="http://healthcare.ai/blog/2016/12/09/technical-necessity-of-healthcareai/" rel="alternate" type="text/html" title="The technical need for healthcare.ai" /><published>2016-12-09T07:47:05-07:00</published><updated>2016-12-09T07:47:05-07:00</updated><id>http://healthcare.ai/blog/2016/12/09/technical-necessity-of-healthcareai</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/09/technical-necessity-of-healthcareai/">&lt;p&gt;Many of you might be wondering how your organization could benefit from healthcare.ai. Even though you’re read the broad statements on the &lt;a href=&quot;http://healthcare.ai&quot;&gt;home page&lt;/a&gt;, you might be asking yourself, “how does healthcare.ai enable my team of analysts or data scientists? And how can it finally bring accurate, informative models to my health system for the first time?”&lt;/p&gt;

&lt;p&gt;While those looking to get into healthcare machine learning (ML) can certainly use R’s &lt;a href=&quot;http://topepo.github.io/caret/index.html&quot;&gt;caret package&lt;/a&gt; or Python’s &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;scikit learn package&lt;/a&gt; to create models, we believe that’s not the most efficient way to spread healthcare ML. These general tools have been around for years, yet ML still hasn’t been broadly adopted in healthcare. The healthcare.ai project helps solve this issue, as it provides a gentle introduction to machine learning and R or Python &lt;em&gt;in healthcare&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;How healthcare.ai helps is that it 1) offers pre-processing and algorithms appropriate for healthcare questions; 2) provides appropriate metrics to assess which algorithm generates the best model; 3) tells you which features (i.e., variables) are most important to your model; 4) provides easy connectivity to databases; and 5) allows you to easily save and deploy a model in production.&lt;/p&gt;

&lt;p&gt;While there are folks in healthcare using R and Python to create models, very few of these ever make it to production. This is partly because 1) it’s hard to produce predictions and interpretations that can actually guide clinicians; 2) it’s grueling to both gather appropriate variables for a model AND write the full ML code to create, assess, and deploy a model; 3) it’s difficult to fix bugs and make updates in a way that &lt;a href=&quot;https://en.wikipedia.org/wiki/Database_administrator&quot;&gt;DBAs&lt;/a&gt; are happy with; and 4) if the model does make it to production, it’s very hard to maintain the custom R/Python code.&lt;/p&gt;

&lt;p&gt;The healthcare.ai packages solve these issues because of our focus on software engineering tools. Our code is under &lt;a href=&quot;https://en.wikipedia.org/wiki/Version_control&quot;&gt;version control&lt;/a&gt;, which allows a team to collaboratively check the code’s robustness, contribute new features, and fix any bugs. We use what are called &lt;a href=&quot;https://en.wikipedia.org/wiki/Unit_testing&quot;&gt;unit tests&lt;/a&gt;, which let us automatically check that all important functionality is working at any time. (This is especially helpful as code changes are made.) As you might have read, we’re also using &lt;a href=&quot;http://r-pkgs.had.co.nz/intro.html&quot;&gt;packages&lt;/a&gt;, which allow teams to easily document code and use &lt;a href=&quot;http://yihui.name/en/2013/06/r-package-versioning/&quot;&gt;versioning&lt;/a&gt; to organize changes in functionality over time. If someone uses healthcare.ai and (now or a year down the road) finds an error, there’s &lt;a href=&quot;http://healthcare.ai/r&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://groups.google.com/forum/#!forum/healthcareai-users&quot;&gt;community&lt;/a&gt; support to help. As we release new versions, the old documentation will still be accessible, in case you haven’t upgraded.&lt;/p&gt;

&lt;p&gt;We want the models you put into production to not only help people today, but for years to come. As people change jobs, new people will naturally inherit responsibility over older models. If the predictive code used in production wasn’t production-grade code (following the above principles) these transitions can be painful and the health system may suffer. We feel that patient outcomes are too important for that to be a common occurrence, which is why we’ve open-sourced healthcare.ai.&lt;/p&gt;

&lt;p&gt;Please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or suggestions!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">Many of you might be wondering how your organization could benefit from healthcare.ai. Even though you’re read the broad statements on the home page, you might be asking yourself, “how does healthcare.ai enable my team of analysts or data scientists? And how can it finally bring accurate, informative models to my health system for the first time?”</summary></entry><entry><title type="html">The benefits of machine learning in healthcare</title><link href="http://healthcare.ai/blog/2016/12/05/benefits-of-machine-learning-in-healthcare/" rel="alternate" type="text/html" title="The benefits of machine learning in healthcare" /><published>2016-12-05T07:24:23-07:00</published><updated>2016-12-05T07:24:23-07:00</updated><id>http://healthcare.ai/blog/2016/12/05/benefits-of-machine-learning-in-healthcare</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/05/benefits-of-machine-learning-in-healthcare/">&lt;p&gt;If you read much about technology, you have likely heard about machine learning, but may be wondering how it would work in healthcare. Where’s the low-hanging fruit? And how could it help my clinical team?&lt;/p&gt;

&lt;p&gt;Throughout healthcare, and many other industries, there are heuristics and established best practices that help people make decisions. A popular example in healthcare is the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2845681/&quot;&gt;LACE index&lt;/a&gt;, which provides the likelihood of patient 30-day readmission risk. You might have also heard of similar tools like the &lt;a href=&quot;http://jamanetwork.com/journals/jama/fullarticle/194262&quot;&gt;SOFA Score&lt;/a&gt;, &lt;a href=&quot;http://www.nejm.org/doi/full/10.1056/NEJM200102153440701#t=article&quot;&gt;Apgar Score&lt;/a&gt;, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2999700/&quot;&gt;PRISM Score&lt;/a&gt;, and the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/9069007&quot;&gt;PIM Score&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Like most of these scores, the &lt;a href=&quot;http://www.besler.com/lace-risk-score/&quot;&gt;LACE calculation&lt;/a&gt; is fairly simple. It’s based on length of stay, acuity of the admission, patient comorbities, and ED visits within the last six months. In each of these categories, points are assigned—a length of stay of three days equals three points, for example. Then the points from each categories are added up to form the LACE index.&lt;/p&gt;

&lt;p&gt;It’s simple and indicative of how healthcare has worked for the last 20-30 years. First, there’s a national study, which eventually leads to guidelines and a simple calculation to help prioritize which patients are most at risk of something.&lt;/p&gt;

&lt;p&gt;So what’s wrong with that? Well, the guidelines can only be &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2845681/&quot;&gt;narrowly applied&lt;/a&gt; and even then &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4670852/&quot;&gt;don’t give impressive results&lt;/a&gt;. Think of it—LACE was developed from patients seen in Ontario from 2004 to 2008. Do your patient demographics closely match those in Ontario? Or, do your patient demographics even match your same set from ten years ago? Perhaps not. Another issue is applicability—since LACE requires the patient’s length of stay, the score is only available upon discharge. What if you want a risk score early during their stay?&lt;/p&gt;

&lt;p&gt;This is why machine learning is fantastic—it fills these gaps. First, it learns the important relationships in your data on past patients and their outcomes. This means that the model is customized on your data from the last few years–you don’t have to rely on scores made on other populations, 10-20 years ago. Second, machine learning allows you to create a model based on whatever data is available when you need a risk score (i.e., upon admission rather than discharge).&lt;/p&gt;

&lt;p&gt;In summary, what does a machine learning model provide? Accurate, timely risk scores, enabling confident and precise resource allocation, leading to lower costs and improved outcomes. As an added bonus, &lt;a href=&quot;http://healthcare.ai/&quot;&gt;healthcare.ai&lt;/a&gt; shows why a risk score was high, so the clinician not only knows which patients are most at risk, but also what can be done to lower that patient’s risk. We’ll detail this ability in a future post.&lt;/p&gt;

&lt;p&gt;Thanks, and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">If you read much about technology, you have likely heard about machine learning, but may be wondering how it would work in healthcare. Where’s the low-hanging fruit? And how could it help my clinical team?</summary></entry><entry><title type="html">Why R and Python?</title><link href="http://healthcare.ai/blog/2016/12/02/why-r-and-python/" rel="alternate" type="text/html" title="Why R and Python?" /><published>2016-12-02T07:17:06-07:00</published><updated>2016-12-02T07:17:06-07:00</updated><id>http://healthcare.ai/blog/2016/12/02/why-r-and-python</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/02/why-r-and-python/">&lt;p&gt;As time goes on, this blog will touch on many of the technical choices made at Health Catalyst. It will mostly focus on data science. If there’s a particular topic that interests, &lt;a href=&quot;http://healthcare.ai/contact.html&quot;&gt;contact us&lt;/a&gt;! Some posts will be short, while others will be in-depth. The tone will be informal, with a focus on content and frequent posts (twice per-week) rather than polish. When we talk about doing things with data, we’ll post the code, so you can follow along.&lt;/p&gt;

&lt;p&gt;When doing healthcare machine learning, why’d we choice R and Python? To be honest, this decision wasn’t that complicated. Of course, there are a lot of fantastic statistical tools available today besides just R and Python: Matlab, SAS, Stata, SPSS, and others. While languages like Java, C++, and C# are great, they’re &lt;a href=&quot;https://en.wikipedia.org/wiki/Compiled_language&quot;&gt;compiled&lt;/a&gt;, which makes them difficult to use for data analysis. We had just a few criteria when narrowing down the list.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Was it open-source?&lt;/li&gt;
  &lt;li&gt;Could it do breadth and depth?&lt;/li&gt;
  &lt;li&gt;Did it have wide support?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We wanted something open-source, because free is obviously great, but also because we wanted to be able to contribute to the same community. Data literacy is important to getting started in most professional careers, and it’s fantastic how free tools have democratized entry into many fields over the last decade–we want to support that.&lt;/p&gt;

&lt;p&gt;R and Python are the obvious open-source options when playing with data, but how are they on breadth and depth as well as community support? Actually pretty fantastic. Python is often known as the Swiss Army knife of programming languages–it can support machine learning, web development, web scraping, desktop applications, etc. It also supports these things well.&lt;/p&gt;

&lt;p&gt;While R is more narrowly focused on statistics compared to Python, it is also great at several things. First, it offers well-documented algorithms and tools for &lt;a href=&quot;https://cran.r-project.org/web/views/&quot;&gt;&lt;em&gt;whatever&lt;/em&gt;&lt;/a&gt; you want to do in statistics. Second, it has fantastic &lt;a href=&quot;http://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html&quot;&gt;visualization software&lt;/a&gt; (better than Python, it could be argued), and thirdly it is great at professional &lt;a href=&quot;http://yihui.name/knitr/&quot;&gt;document generation&lt;/a&gt; for both reports and data education.&lt;/p&gt;

&lt;p&gt;In terms of support, here’s a plot showing the popularity of each of these languages over time on &lt;a href=&quot;http://stackoverflow.com/&quot;&gt;Stack Overflow&lt;/a&gt;, a popular Q and A site:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/RvsPyPost_LanguageComparisonOverTime.png&quot; alt=&quot;Popularity Plot&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While there are many other options for doing statistics in healthcare, R and Python are among the very best, and we’re excited to use them to improve patient care.&lt;/p&gt;

&lt;p&gt;Note: We obtained data using &lt;a href=&quot;https://data.stackexchange.com/stackoverflow/query/596780/language-trends-questions-per-tag-per-month&quot;&gt;this tool&lt;/a&gt;; we &lt;a href=&quot;https://gist.github.com/levithatcher/130ee5d6586839ceeb3975e0afee9b65&quot;&gt;plotted using R&lt;/a&gt;. For comparison, note that for November 2016 the Stack Overflow question break-down was Python (16,759); R (4,346); Matlab (1,101); SAS (190); Stata (46); SPSS (25); and JMP (2), though the Python numbers are inflated, as it’s used for much more than statistics.&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">As time goes on, this blog will touch on many of the technical choices made at Health Catalyst. It will mostly focus on data science. If there’s a particular topic that interests, contact us! Some posts will be short, while others will be in-depth. The tone will be informal, with a focus on content and frequent posts (twice per-week) rather than polish. When we talk about doing things with data, we’ll post the code, so you can follow along.</summary></entry></feed>
