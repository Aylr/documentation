<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://healthcare.ai/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.3.1">Jekyll</generator><link href="http://healthcare.ai/feed.xml" rel="self" type="application/atom+xml" /><link href="http://healthcare.ai/" rel="alternate" type="text/html" /><updated>2016-12-21T20:51:08-07:00</updated><id>http://healthcare.ai//</id><title type="html">healthcare.ai</title><entry><title type="html">Which algorithms are in healthcare.ai?</title><link href="http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/" rel="alternate" type="text/html" title="Which algorithms are in healthcare.ai?" /><published>2016-12-21T20:17:11-07:00</published><updated>2016-12-21T20:17:11-07:00</updated><id>http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/21/which-algorithms-are-in-healthcareai/">&lt;p&gt;Machine learning has been around for decades and has been used to solve lots of problems. Some of these include &lt;a href=&quot;http://ats.cs.ut.ee/u/kt/hw/spam/spam.pdf&quot;&gt;spam filtering&lt;/a&gt; for email, &lt;a href=&quot;http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html&quot;&gt;suggestions&lt;/a&gt; on Netflix, &lt;a href=&quot;http://qz.com/571007/the-magic-that-makes-spotifys-discover-weekly-playlists-so-damn-good/&quot;&gt;optimized playlists&lt;/a&gt; on Spotify, &lt;a href=&quot;https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf&quot;&gt;custom recommendations&lt;/a&gt; on Amazon, &lt;a href=&quot;https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78#.7b9c9jmg7&quot;&gt;facial recognition&lt;/a&gt; on Facebook, &lt;a href=&quot;https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html&quot;&gt;voice recognition&lt;/a&gt; on your phone, &lt;a href=&quot;http://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html&quot;&gt;language translation&lt;/a&gt; on demand, &lt;a href=&quot;http://fusion.net/story/142326/the-new-google-photos-app-is-disturbingly-good-at-data-mining-your-photos/&quot;&gt;image search&lt;/a&gt; in your photo app, and &lt;a href=&quot;https://www.kaggle.com/wiki/DataScienceUseCases&quot;&gt;many more&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While reading that long and varied list, you may be wondering where healthcare stands by comparison. Even though machine learning can solve many problems in healthcare, the field has not yet seen significant adoption. As &lt;a href=&quot;http://healthcare.ai/blog/2016/12/01/welcome-to-healthcareai/&quot;&gt;we’ve mentioned&lt;/a&gt;, the goal of healthcare.ai is to change that.&lt;/p&gt;

&lt;p&gt;We plan to bring the benefits of machine learning (ML) into healthcare by starting with the low-hanging fruit. Health Catalyst is a very practical company and that is reflected in &lt;a href=&quot;http://healthcare.ai/&quot;&gt;healthcare.ai&lt;/a&gt;. While many of the machine learning projects mentioned above are using advanced algorithms like &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;deep learning&lt;/a&gt;, healthcare.ai is instead starting with the workhorses of the algorithm world. Note that this a different approach to healthcare ML compared to that of &lt;a href=&quot;https://research.google.com/teams/brain/healthcare/&quot;&gt;Google&lt;/a&gt; and &lt;a href=&quot;http://searchhealthit.techtarget.com/opinion/Microsoft-Project-Adam-may-reach-healthcare-specialties&quot;&gt;Microsoft&lt;/a&gt;, which are focusing on the sexier (but less practical) deep learning applications in healthcare.&lt;/p&gt;

&lt;p&gt;Before starting the discussion on healthcare.ai algorithm choices, I should note that we’ll focus on our R package and on &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_classification&quot;&gt;classification&lt;/a&gt; (rather than regression), since &lt;em&gt;most&lt;/em&gt; problems in healthcare revolve around predicting &lt;a href=&quot;http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created/&quot;&gt;Yes or No&lt;/a&gt; rather than a continuous variable. To make the terminology clear, it should also be stated that a machine learning algorithm, when paired with data, leads to a model. The algorithms exist off the shelf. Their value lies in the details of their implementation with a specific dataset. Healthcare.ai has open-sourced tools that allow you to easily match your data with suitable algorithms, create models, and help you answer your most important business questions. The models that you and Health Catalyst create are proprietary, but the tools used to make those models are free.&lt;/p&gt;

&lt;p&gt;Arguably the most simple and common algorithm when trying to classify things via machine learning is &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;logistic regression&lt;/a&gt; (note that despite the name, this algorithm is used for classification problems). We love it because it’s fairly easy to use, fairly quick to finish, and fairly easy to interpret. We’ve been using this algorithm, but with a twist to it.&lt;/p&gt;

&lt;p&gt;We wanted users to have guidance as to which &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;features&lt;/a&gt; (i.e., variables) were predictive and worth using when building a model. This drove us to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Lasso_(statistics)&quot;&gt;lasso&lt;/a&gt;, which is a linear model much like logistic regression, but will provide feedback on which features should or shouldn’t be included in the model. You might say, “Well, couldn’t logistic regression just ignore features that weren’t predictive?” Yes, but when the user has brought 20-40 variables into a focused dataset—what Health Catalyst calls a source area mart—to see if they help predict Sepsis, they’ll often only want to keep those variables that are predictive, as ETL processes often have hard resource constraints. With knowledge of how important each feature is, one can often remove many non-predictive variables from a model without any significant loss in accuracy.&lt;/p&gt;

&lt;p&gt;Next to linear models like lasso and logistic regression, the most common algorithm in machine learning is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;&gt;random forest&lt;/a&gt;. It’s different in that it can model non-linear relationships accurately. The random forest algorithm is an ensemble method, which aggregates the result of 100+ randomized decision trees to produce a prediction. While it’s a little harder to interpret than linear algorithms (like lasso), it typically doesn’t need much &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;&gt;hyperparameter tuning&lt;/a&gt;, can run quite quickly, and from our experience often provides more accurate models (compared to linear algorithms) for common healthcare questions.&lt;/p&gt;

&lt;p&gt;To back up a bit, in typical machine learning problems row order doesn’t matter. If you think of the simple example of using housing data in Salt Lake City to determine the relationship between square footage and house price, it doesn’t matter if the row for house 10045 was listed before house 10057 in the dataset. Those two rows are treated independently. In longitudinal datasets however, the relationship between the rows often &lt;em&gt;does&lt;/em&gt; matter. In certain clinical datasets, we often find multiple entries for the same person, showing the person’s progression over time. When this progression is important to the business question that’s being addressed with machine learning, basic machine learning might not be suitable.&lt;/p&gt;

&lt;p&gt;This frequent longitudinal aspect to healthcare data is why we’re excited to offer linear &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixed_model&quot;&gt;mixed models&lt;/a&gt; in our the R version of healthcare.ai. Mixed models offer the ability to combine a personal trend with a population trend. It’s called a mixed model because it combines fixed effects (i.e., those that relate to the population as a whole) with random effects (i.e., those that are innate to that individual). &lt;a href=&quot;http://www.bodowinter.com/tutorial/bw_LME_tutorial.pdf&quot;&gt;This paper&lt;/a&gt; provides a good introduction to the topic. From our experience, this algorithm is slow compared to random forest and lasso, but can create a better model if the prediction at hand significantly depends on a person’s history (i.e., think diabetic amputation risk rather than &lt;a href=&quot;https://en.wikipedia.org/wiki/Central_venous_catheter#Bloodstream_infections&quot;&gt;CLABSI&lt;/a&gt;). As always, healthcare.ai makes it easy to see how this algorithm performs on your dataset, and determine if it does a better job than the more common lasso and random forest algorithms.&lt;/p&gt;

&lt;p&gt;Again, the goal of healthcare.ai is to help the medical community use machine learning to improve healthcare outcomes. On the first pass, we have implemented what we feel are the simplest and most effective algorithms specific to healthcare data. We’ll certainly expand in the future, but for now there are a lot of efficiency gains to be &lt;achieved&gt;&lt;/achieved&gt; with basic algorithms, smart implementation, and centralized documentation.&lt;/p&gt;

&lt;p&gt;If you want more detail, check out &lt;a href=&quot;https://github.com/HealthCatalystSLC/healthcareai-r&quot;&gt;our code&lt;/a&gt;. If you have questions or feedback, &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;contact us&lt;/a&gt;!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="algorithms" /><summary type="html">Machine learning has been around for decades and has been used to solve lots of problems. Some of these include spam filtering for email, suggestions on Netflix, optimized playlists on Spotify, custom recommendations on Amazon, facial recognition on Facebook, voice recognition on your phone, language translation on demand, image search in your photo app, and many more.</summary></entry><entry><title type="html">Model evaluation using ROC Curves</title><link href="http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/" rel="alternate" type="text/html" title="Model evaluation using ROC Curves" /><published>2016-12-15T15:37:00-07:00</published><updated>2016-12-15T15:37:00-07:00</updated><id>http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/15/model-evaluation-using-roc-curves/">&lt;p&gt;Before a new technique in healthcare can be introduced to patient use, it must pass a rigorous set of quality standards. Then, to actually be adopted and see widespread use, a technique must be trusted and accepted by physicians and other front line care workers. For example, new drugs are evaluated in several steps before making into human trials, and then still have several hurdles to clear before they can be accepted as standard of care. Machine learning is poised to make a significant impact in clinical care in the near future, but it is not exempt from these same checks and developmental hurdles.&lt;/p&gt;

&lt;p&gt;The main goal of the healthcare.ai is to improve healthcare outcomes. As detailed in a previous post, we provide the tools and models that can use existing data to help intelligently guide clinical decisions. There has to be trust and transparency in these models if they are to make an impact and see long-term adoption. Just like a new drug, every model we build is evaluated to make sure that it’s high-quality before it is pushed into production. We evaluate models to compare them with other techniques, know when in their development they are ready for production, and to get an overall sense of how much we should trust them. Interestingly enough, one common way to do this (at least in classification problems) borrows from other areas of medicine and uses a &lt;em&gt;Receiver Operating Characteristic Curve (ROC).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Before we can get to the curve itself, we need a few definitions. Let’s say we’ve generated a machine learning model to predict the likelihood of 30-day readmission in a set of patients. The model gives a probability (between 0 and 1) for each person of how likely they are to be readmitted. 30 days later, the &lt;em&gt;True Positive Rate (TPR)&lt;/em&gt; is the proportion of actual readmissions that the test correctly predicted would be readmitted. The &lt;em&gt;False Positive Rate (FPR)&lt;/em&gt; is the proportion of patients whom the model predicted would be readmitted, but were not. In order to make these black and white predictions, we must pick a decision boundary somewhere between 0 and 1. Remember, the model gives a probability, not a definitive answer. If we were to choose 0.9, we would say that everyone with readmittance probability above 0.9 is a readmission, everyone below is not. We could then calculate the TPR and FPR, and have a measure of how well our model performed at 0.9 decision boundary. As you might have guessed, the decision boundary is a sticky spot. If we were choose 0.8 to increase the TPR, it will come at the expense of a larger FPR. The three parameters are tied to one another in a way that makes models hard to interpret and discuss.&lt;/p&gt;

&lt;p&gt;The ROC is a common way to avoid this. It is a graphical representation of the balance between TPR and FPR at &lt;em&gt;every&lt;/em&gt; possible decision boundary. The Area Under the Curve (AUC) is that magic solution that we have been looking for. The AUC is a single number that can evaluate a model’s performance, regardless of the chosen decision boundary. The perfect machine learning model will have an AUC of 1.0 (cyan), while a random one will have an AUC of 0.5 (orange). A good model will be over 0.7, a great one will be over 0.85. It might not be possible to perfectly classify a data set, but the AUC is a good way to compare models on that data, across patient cohorts, and give a sense of how trustworthy that model is in general.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/AUCPost_ROCExample.png&quot; alt=&quot;Example ROC Curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Whenever we are getting ready to deploy a model into use, we need to evaluate its overall performance. The AUC gives us a transparent, easy-to-interpret way to do that. Of course, it has limitations. For example, the usefulness of the ROC curve begins to break down with heavily imbalanced classes, obviously a big problem for healthcare data. One solution is to use AUC from a Precision-Recall Curve, but we’ll save that for a future post. If you’re interested in trying out ROC curves on your data, you’ll find some handy tools already built into the healthcare.ai package to help you evalutate your models. Finally, if you’re hungry for more, there are many &lt;a href=&quot;https://classeval.wordpress.com/introduction/introduction-to-the-roc-receiver-operating-characteristics-plot/&quot;&gt;great tutorials online&lt;/a&gt; for ROC curves.&lt;/p&gt;

&lt;p&gt;Thanks for reading and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Mike Mastanduno</name></author><category term="overview" /><summary type="html">This blog will describe the motivation and uses of the ROC curve</summary></entry><entry><title type="html">What models has Health Catalyst created with healthcare.ai?</title><link href="http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created/" rel="alternate" type="text/html" title="What models has Health Catalyst created with healthcare.ai?" /><published>2016-12-12T09:28:05-07:00</published><updated>2016-12-12T09:28:05-07:00</updated><id>http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/12/what-models-has-health-catalyst-created/">&lt;p&gt;After reading a few articles on healthcare.ai, some of you may be saying, well, that’s great–but what has Health Catalyst actually used it for? Since Health Catalyst has been open with sharing the tool set, it only makes sense that they’d also be willing to share details of its use. As the Director of Data Science at Health Catalyst and founder of healthcare.ai, I oversee all client predictive engagements, and will make a point of frequently updating the community on our work. If you have questions, comments, or criticism, please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The goal of healthcare.ai was to provide a simple, flexible tool to streamline healthcare machine learning. This means that it works across financial, operational, and clinical realms. If a health system has a business question that they want predictions for, we will make healthcare.ai flexible enough to cover that use case. Today we’ll briefly cover three recent predictive project, and detail more for a future post.&lt;/p&gt;

&lt;p&gt;Let’s start with finance. Uncompensated care is a growing problem at most health systems. To help a counter this trend, we’ve started creating propensity-to-pay models. Recall that each health system interested in using machine learning is provided a custom model, tailored to their data. In this propensity-to-pay project, for each person with an open account with the health system, each month the probability of payment is calculated. This personal probability can be used to determine 1) who may need reminders, 2) who may need financial assistance, and 3) how the likelihood of payment changes over time and after particular life events. A few &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;features&lt;/a&gt; that were important to this model turned were things like whether the person paid last month (surprise!), account balance, a person’s age, the month of the year, etc. These may vary for your health system (as your demographics are likely a bit different), but healthcare.ai makes it easy to customize the model to &lt;em&gt;your&lt;/em&gt; payment data.&lt;/p&gt;

&lt;p&gt;Those who have ever worked in a clinical setting know how hard it is to maintain schedules that keep both clinicians and patients happy. Slots are often over or under-booked because someone showed up late, didn’t show up, or showed up without warning. Health Catalyst taken a first pass at this problem via a no-show predictive model. In this engagement, we gathered all past data on the characteristics of people that had showed or hadn’t showed for their appointments, and created an accurate predictive model to assess, with each scheduled appointment, the risk of a no-show. If the clinic feels that they can reduce their no-show rate by extra phone calls (or other measures), this predictive guidnaces assures that the resources used are efficiently allocated. If, on the other hand, the clinic has found that it’s quite hard to reduce the no-show rate (even with this guidance), they can use the probability scores to over-book particular slots, such that clinical scheduling is optimized using past data. &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;Features&lt;/a&gt; that were particularly helpful in this prediction were prior number of cancellations, appointment type, and week of the year.&lt;/p&gt;

&lt;p&gt;Finally, we’ll touch on a clinical case. Many health systems are &lt;a href=&quot;http://www.modernhealthcare.com/article/20150803/NEWS/150809981&quot;&gt;penalized&lt;/a&gt; if their 30-day readmissions rate is too high. While general readmissions models are possible via healthcare.ai, we’ve found that focusing on particular disease cohorts (such as for heart failure, sepsis, or COPD) allows us to create a much more accurate model. We’ve had multiple engagements recently related to 30 and 90-day COPD readmissions. How it works is that the relevant data on past patients (and whether they had a readmission or not) is collected into a &lt;a href=&quot;https://www.healthcatalyst.com/late-binding-data-warehouse/late-binding-data-bus/sam-designer/&quot;&gt;subject area mart&lt;/a&gt;, and a couple of different algorithms are used to create models. Once we find the column set and algorithm that together produce the most accurate model, we save the model and integrate it into our nightly &lt;a href=&quot;https://en.wikipedia.org/wiki/Extract,_transform,_load&quot;&gt;ETL&lt;/a&gt;. This way, clinicians  receive daily guidance as to which of their patients is most likely to be readmitted. Among others, &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_(machine_learning)&quot;&gt;features&lt;/a&gt; like prior readmissions, pre-existing conditions, and specific facility were particularly helpful to this model.&lt;/p&gt;

&lt;p&gt;Considering the resource constraints present in many hospital units, this type of machine learning guidance can be crucial to efficiently deploying resources toward achieving business goals (i.e., reducing readmissions, reducing 1-yr mortality, preventing &lt;a href=&quot;https://www.cdc.gov/hai/&quot;&gt;HAIs&lt;/a&gt;, etc). As time goes on, we’ll detail more of these predictive projects, and explain how they might be useful to your health system.&lt;/p&gt;

&lt;p&gt;Thanks, and please &lt;a href=&quot;http://healthcare.ai/contact.html&quot;&gt;reach out&lt;/a&gt; with any questions!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">After reading a few articles on healthcare.ai, some of you may be saying, well, that’s great–but what has Health Catalyst actually used it for? Since Health Catalyst has been open with sharing the tool set, it only makes sense that they’d also be willing to share details of its use. As the Director of Data Science at Health Catalyst and founder of healthcare.ai, I oversee all client predictive engagements, and will make a point of frequently updating the community on our work. If you have questions, comments, or criticism, please reach out.</summary></entry><entry><title type="html">The technical need for healthcare.ai</title><link href="http://healthcare.ai/blog/2016/12/09/technical-necessity-of-healthcareai/" rel="alternate" type="text/html" title="The technical need for healthcare.ai" /><published>2016-12-09T07:47:05-07:00</published><updated>2016-12-09T07:47:05-07:00</updated><id>http://healthcare.ai/blog/2016/12/09/technical-necessity-of-healthcareai</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/09/technical-necessity-of-healthcareai/">&lt;p&gt;Many of you might be wondering how your organization could benefit from healthcare.ai. Even though you’re read the broad statements on the &lt;a href=&quot;http://healthcare.ai&quot;&gt;home page&lt;/a&gt;, you might be asking yourself, “how does healthcare.ai enable my team of analysts or data scientists? And how can it finally bring accurate, informative models to my health system for the first time?”&lt;/p&gt;

&lt;p&gt;While those looking to get into healthcare machine learning (ML) can certainly use R’s &lt;a href=&quot;http://topepo.github.io/caret/index.html&quot;&gt;caret package&lt;/a&gt; or Python’s &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;scikit learn package&lt;/a&gt; to create models, we believe that’s not the most efficient way to spread healthcare ML. These general tools have been around for years, yet ML still hasn’t been broadly adopted in healthcare. The healthcare.ai project helps solve this issue, as it provides a gentle introduction to machine learning and R or Python &lt;em&gt;in healthcare&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;How healthcare.ai helps is that it 1) offers pre-processing and algorithms appropriate for healthcare questions; 2) provides appropriate metrics to assess which algorithm generates the best model; 3) tells you which features (i.e., variables) are most important to your model; 4) provides easy connectivity to databases; and 5) allows you to easily save and deploy a model in production.&lt;/p&gt;

&lt;p&gt;While there are folks in healthcare using R and Python to create models, very few of these ever make it to production. This is partly because 1) it’s hard to produce predictions and interpretations that can actually guide clinicians; 2) it’s grueling to both gather appropriate variables for a model AND write the full ML code to create, assess, and deploy a model; 3) it’s difficult to fix bugs and make updates in a way that &lt;a href=&quot;https://en.wikipedia.org/wiki/Database_administrator&quot;&gt;DBAs&lt;/a&gt; are happy with; and 4) if the model does make it to production, it’s very hard to maintain the custom R/Python code.&lt;/p&gt;

&lt;p&gt;The healthcare.ai packages solve these issues because of our focus on software engineering tools. Our code is under &lt;a href=&quot;https://en.wikipedia.org/wiki/Version_control&quot;&gt;version control&lt;/a&gt;, which allows a team to collaboratively check the code’s robustness, contribute new features, and fix any bugs. We use what are called &lt;a href=&quot;https://en.wikipedia.org/wiki/Unit_testing&quot;&gt;unit tests&lt;/a&gt;, which let us automatically check that all important functionality is working at any time. (This is especially helpful as code changes are made.) As you might have read, we’re also using &lt;a href=&quot;http://r-pkgs.had.co.nz/intro.html&quot;&gt;packages&lt;/a&gt;, which allow teams to easily document code and use &lt;a href=&quot;http://yihui.name/en/2013/06/r-package-versioning/&quot;&gt;versioning&lt;/a&gt; to organize changes in functionality over time. If someone uses healthcare.ai and (now or a year down the road) finds an error, there’s &lt;a href=&quot;http://healthcare.ai/r&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://groups.google.com/forum/#!forum/healthcareai-users&quot;&gt;community&lt;/a&gt; support to help. As we release new versions, the old documentation will still be accessible, in case you haven’t upgraded.&lt;/p&gt;

&lt;p&gt;We want the models you put into production to not only help people today, but for years to come. As people change jobs, new people will naturally inherit responsibility over older models. If the predictive code used in production wasn’t production-grade code (following the above principles) these transitions can be painful and the health system may suffer. We feel that patient outcomes are too important for that to be a common occurrence, which is why we’ve open-sourced healthcare.ai.&lt;/p&gt;

&lt;p&gt;Please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or suggestions!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">Many of you might be wondering how your organization could benefit from healthcare.ai. Even though you’re read the broad statements on the home page, you might be asking yourself, “how does healthcare.ai enable my team of analysts or data scientists? And how can it finally bring accurate, informative models to my health system for the first time?”</summary></entry><entry><title type="html">The benefits of machine learning in healthcare</title><link href="http://healthcare.ai/blog/2016/12/05/benefits-of-machine-learning-in-healthcare/" rel="alternate" type="text/html" title="The benefits of machine learning in healthcare" /><published>2016-12-05T07:24:23-07:00</published><updated>2016-12-05T07:24:23-07:00</updated><id>http://healthcare.ai/blog/2016/12/05/benefits-of-machine-learning-in-healthcare</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/05/benefits-of-machine-learning-in-healthcare/">&lt;p&gt;If you read much about technology, you have likely heard about machine learning, but may be wondering how it would work in healthcare. Where’s the low-hanging fruit? And how could it help my clinical team?&lt;/p&gt;

&lt;p&gt;Throughout healthcare, and many other industries, there are heuristics and established best practices that help people make decisions. A popular example in healthcare is the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2845681/&quot;&gt;LACE index&lt;/a&gt;, which provides the likelihood of patient 30-day readmission risk. You might have also heard of similar tools like the &lt;a href=&quot;http://jamanetwork.com/journals/jama/fullarticle/194262&quot;&gt;SOFA Score&lt;/a&gt;, &lt;a href=&quot;http://www.nejm.org/doi/full/10.1056/NEJM200102153440701#t=article&quot;&gt;Apgar Score&lt;/a&gt;, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2999700/&quot;&gt;PRISM Score&lt;/a&gt;, and the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/9069007&quot;&gt;PIM Score&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Like most of these scores, the &lt;a href=&quot;http://www.besler.com/lace-risk-score/&quot;&gt;LACE calculation&lt;/a&gt; is fairly simple. It’s based on length of stay, acuity of the admission, patient comorbities, and ED visits within the last six months. In each of these categories, points are assigned—a length of stay of three days equals three points, for example. Then the points from each categories are added up to form the LACE index.&lt;/p&gt;

&lt;p&gt;It’s simple and indicative of how healthcare has worked for the last 20-30 years. First, there’s a national study, which eventually leads to guidelines and a simple calculation to help prioritize which patients are most at risk of something.&lt;/p&gt;

&lt;p&gt;So what’s wrong with that? Well, the guidelines can only be &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2845681/&quot;&gt;narrowly applied&lt;/a&gt; and even then &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4670852/&quot;&gt;don’t give impressive results&lt;/a&gt;. Think of it—LACE was developed from patients seen in Ontario from 2004 to 2008. Do your patient demographics closely match those in Ontario? Or, do your patient demographics even match your same set from ten years ago? Perhaps not. Another issue is applicability—since LACE requires the patient’s length of stay, the score is only available upon discharge. What if you want a risk score early during their stay?&lt;/p&gt;

&lt;p&gt;This is why machine learning is fantastic—it fills these gaps. First, it learns the important relationships in your data on past patients and their outcomes. This means that the model is customized on your data from the last few years–you don’t have to rely on scores made on other populations, 10-20 years ago. Second, machine learning allows you to create a model based on whatever data is available when you need a risk score (i.e., upon admission rather than discharge).&lt;/p&gt;

&lt;p&gt;In summary, what does a machine learning model provide? Accurate, timely risk scores, enabling confident and precise resource allocation, leading to lower costs and improved outcomes. As an added bonus, &lt;a href=&quot;http://healthcare.ai/&quot;&gt;healthcare.ai&lt;/a&gt; shows why a risk score was high, so the clinician not only knows which patients are most at risk, but also what can be done to lower that patient’s risk. We’ll detail this ability in a future post.&lt;/p&gt;

&lt;p&gt;Thanks, and please &lt;a href=&quot;http://healthcare.ai/contact&quot;&gt;reach out&lt;/a&gt; with any questions or comments!&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">If you read much about technology, you have likely heard about machine learning, but may be wondering how it would work in healthcare. Where’s the low-hanging fruit? And how could it help my clinical team?</summary></entry><entry><title type="html">Why R and Python?</title><link href="http://healthcare.ai/blog/2016/12/02/why-r-and-python/" rel="alternate" type="text/html" title="Why R and Python?" /><published>2016-12-02T07:17:06-07:00</published><updated>2016-12-02T07:17:06-07:00</updated><id>http://healthcare.ai/blog/2016/12/02/why-r-and-python</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/02/why-r-and-python/">&lt;p&gt;As time goes on, this blog will touch on many of the technical choices made at Health Catalyst. It will mostly focus on data science. If there’s a particular topic that interests, &lt;a href=&quot;http://healthcare.ai/contact.html&quot;&gt;contact us&lt;/a&gt;! Some posts will be short, while others will be in-depth. The tone will be informal, with a focus on content and frequent posts (twice per-week) rather than polish. When we talk about doing things with data, we’ll post the code, so you can follow along.&lt;/p&gt;

&lt;p&gt;When doing healthcare machine learning, why’d we choice R and Python? To be honest, this decision wasn’t that complicated. Of course, there are a lot of fantastic statistical tools available today besides just R and Python: Matlab, SAS, Stata, SPSS, and others. While languages like Java, C++, and C# are great, they’re &lt;a href=&quot;https://en.wikipedia.org/wiki/Compiled_language&quot;&gt;compiled&lt;/a&gt;, which makes them difficult to use for data analysis. We had just a few criteria when narrowing down the list.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Was it open-source?&lt;/li&gt;
  &lt;li&gt;Could it do breadth and depth?&lt;/li&gt;
  &lt;li&gt;Did it have wide support?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We wanted something open-source, because free is obviously great, but also because we wanted to be able to contribute to the same community. Data literacy is important to getting started in most professional careers, and it’s fantastic how free tools have democratized entry into many fields over the last decade–we want to support that.&lt;/p&gt;

&lt;p&gt;R and Python are the obvious open-source options when playing with data, but how are they on breadth and depth as well as community support? Actually pretty fantastic. Python is often known as the Swiss Army knife of programming languages–it can support machine learning, web development, web scraping, desktop applications, etc. It also supports these things well.&lt;/p&gt;

&lt;p&gt;While R is more narrowly focused on statistics compared to Python, it is also great at several things. First, it offers well-documented algorithms and tools for &lt;a href=&quot;https://cran.r-project.org/web/views/&quot;&gt;&lt;em&gt;whatever&lt;/em&gt;&lt;/a&gt; you want to do in statistics. Second, it has fantastic &lt;a href=&quot;http://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html&quot;&gt;visualization software&lt;/a&gt; (better than Python, it could be argued), and thirdly it is great at professional &lt;a href=&quot;http://yihui.name/knitr/&quot;&gt;document generation&lt;/a&gt; for both reports and data education.&lt;/p&gt;

&lt;p&gt;In terms of support, here’s a plot showing the popularity of each of these languages over time on &lt;a href=&quot;http://stackoverflow.com/&quot;&gt;Stack Overflow&lt;/a&gt;, a popular Q and A site:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/RvsPyPost_LanguageComparisonOverTime.png&quot; alt=&quot;Popularity Plot&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While there are many other options for doing statistics in healthcare, R and Python are among the very best, and we’re excited to use them to improve patient care.&lt;/p&gt;

&lt;p&gt;Note: We obtained data using &lt;a href=&quot;https://data.stackexchange.com/stackoverflow/query/596780/language-trends-questions-per-tag-per-month&quot;&gt;this tool&lt;/a&gt;; we &lt;a href=&quot;https://gist.github.com/levithatcher/130ee5d6586839ceeb3975e0afee9b65&quot;&gt;plotted using R&lt;/a&gt;. For comparison, note that for November 2016 the Stack Overflow question break-down was Python (16,759); R (4,346); Matlab (1,101); SAS (190); Stata (46); SPSS (25); and JMP (2), though the Python numbers are inflated, as it’s used for much more than statistics.&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">As time goes on, this blog will touch on many of the technical choices made at Health Catalyst. It will mostly focus on data science. If there’s a particular topic that interests, contact us! Some posts will be short, while others will be in-depth. The tone will be informal, with a focus on content and frequent posts (twice per-week) rather than polish. When we talk about doing things with data, we’ll post the code, so you can follow along.</summary></entry><entry><title type="html">Welcome to healthcare.ai</title><link href="http://healthcare.ai/blog/2016/12/01/welcome-to-healthcareai/" rel="alternate" type="text/html" title="Welcome to healthcare.ai" /><published>2016-12-01T08:37:13-07:00</published><updated>2016-12-01T08:37:13-07:00</updated><id>http://healthcare.ai/blog/2016/12/01/welcome-to-healthcareai</id><content type="html" xml:base="http://healthcare.ai/blog/2016/12/01/welcome-to-healthcareai/">&lt;p&gt;Health Catalyst’s data science team is excited to present &lt;a href=&quot;http://healthcare.ai/&quot;&gt;healthcare.ai&lt;/a&gt;. This ambitious new project offers healthcare-specific machine learning packages, as well as analysis, commentary, and advice on leveraging machine learning within any health system, regardless of size.&lt;/p&gt;

&lt;p&gt;While companies like &lt;a href=&quot;https://research.google.com/teams/brain/healthcare/&quot;&gt;Google&lt;/a&gt;, &lt;a href=&quot;http://searchhealthit.techtarget.com/opinion/Microsoft-Project-Adam-may-reach-healthcare-specialties&quot;&gt;Microsoft&lt;/a&gt;, and &lt;a href=&quot;https://www.mskcc.org/about/innovative-collaborations/watson-oncology&quot;&gt;IBM&lt;/a&gt; are doing machine learning on the outskirts of healthcare, we work from the center. We bring practical, accurate predictive models to health systems interested in improving their operational, financial, and clinical efficiencies. First and foremost, we want to improve patient outcomes.&lt;/p&gt;

&lt;p&gt;Machine learning has been used in other industries for years, but healthcare is late to the game. We want to change this. Considering the enormous amount of data in EMRs, the impressive ability of machine learning to detect complex patterns, and the thousands of &lt;a href=&quot;http://journals.lww.com/journalpatientsafety/Fulltext/2013/09000/A_New,_Evidence_based_Estimate_of_Patient_Harms.2.aspx&quot;&gt;lives at risk&lt;/a&gt; in hospitals each day, it’s imperative to help healthcare organizations use data to make better decisions.&lt;/p&gt;

&lt;p&gt;Instead of health systems hiring expensive data scientists to do this work, we want to help anyone interested in healthcare data—BI Developers and SQL-based folks in particular—develop machine learning skills (via R or Python).&lt;/p&gt;

&lt;p&gt;We’ll share more details about healthcare.ai in the coming weeks, but here’s a snapshot of what it encompasses:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A healthcare-specific R machine learning package&lt;/li&gt;
  &lt;li&gt;A healthcare-specific Python machine learning package&lt;/li&gt;
  &lt;li&gt;Guidance on where and how to apply machine learning in healthcare&lt;/li&gt;
  &lt;li&gt;Analysis on outcomes improvement via CAFE, the largest healthcare dataset ever compiled&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get started with healthcare machine learning now, see &lt;a href=&quot;http://healthcare.ai/&quot;&gt;healthcare.ai&lt;/a&gt; for R and Python.&lt;/p&gt;</content><author><name>Levi Thatcher</name></author><category term="overview" /><summary type="html">This blog will describe technical choices made at Health Catalyst</summary></entry></feed>
