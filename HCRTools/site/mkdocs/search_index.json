{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to HCRTools\n\n\nThis package will get you started with healthcare machine learning in R.\n\n\nWhat can you do with it?\n\n\n\n\nCompare models based on your data.\n\n\nSave and deploy a model.\n\n\nPerform risk-adjusted comparisons.\n\n\nDo trend analysis following \nNelson rules\n.\n\n\nImprove sparse data via longitudinal imputation.\n\n\n\n\n\n\nHow is it specific to healthcare?\n\n\n\n\nLongitudinal imputation\n\n\nRisk-adjusted comparisons\n\n\nA focus on SQL Server\n\n\n\n\n\n\nHow to install\n\n\nWork in the console of RStudio or RGui\n\n\n\n\nGrab prerequisites\n\n\n\n\ninstall.packages(c('caret','data.table','devtools','doParallel','e1071','grpreg','lubridate',\n'pROC','R6','ranger','ROCR','RODBC'),repos = \nhttps://cran.cnr.berkeley.edu/\n)\n\n\n\n\n\n\nInstall HCRTools\n\n\n\n\nlibrary(devtools)\ndevtools::install_github(repo='HealthCatalystSLC/HCRTools')",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-hcrtools",
            "text": "This package will get you started with healthcare machine learning in R.",
            "title": "Welcome to HCRTools"
        },
        {
            "location": "/#what-can-you-do-with-it",
            "text": "Compare models based on your data.  Save and deploy a model.  Perform risk-adjusted comparisons.  Do trend analysis following  Nelson rules .  Improve sparse data via longitudinal imputation.",
            "title": "What can you do with it?"
        },
        {
            "location": "/#how-is-it-specific-to-healthcare",
            "text": "Longitudinal imputation  Risk-adjusted comparisons  A focus on SQL Server",
            "title": "How is it specific to healthcare?"
        },
        {
            "location": "/#how-to-install",
            "text": "Work in the console of RStudio or RGui   Grab prerequisites   install.packages(c('caret','data.table','devtools','doParallel','e1071','grpreg','lubridate',\n'pROC','R6','ranger','ROCR','RODBC'),repos =  https://cran.cnr.berkeley.edu/ )   Install HCRTools   library(devtools)\ndevtools::install_github(repo='HealthCatalystSLC/HCRTools')",
            "title": "How to install"
        },
        {
            "location": "/getting-started/where-do-i-begin/",
            "text": "Where do I begin with HCRTools?\n\n\nThat, of course, depends on what you want to accomplish:\n\n\nIf you want create a model on your data via machine learning\n\n\n\n\nLook at the \npre-processing tools\n that might help your model. \n\n\nLook at the \nmodel development page\n to create and compare models.\n\n\nLook at the \ndeployment page\n to deploy model.\n\n\n\n\nIf you want to do general healthcare statistics\n\n\n\n\nLook \nhere\n to calculate cross-column correlations on your data.\n\n\nLook \nhere\n to perform trend analysis across your data following Nelson rules.\n\n\n\n\nIf you want to do risk-adjusted comparisons\n\n\n\n\nLook \nhere\n if you want to compare several hospitals or units in terms of risk-adjusted performance.",
            "title": "Where do I begin?"
        },
        {
            "location": "/getting-started/where-do-i-begin/#where-do-i-begin-with-hcrtools",
            "text": "That, of course, depends on what you want to accomplish:",
            "title": "Where do I begin with HCRTools?"
        },
        {
            "location": "/getting-started/where-do-i-begin/#if-you-want-create-a-model-on-your-data-via-machine-learning",
            "text": "Look at the  pre-processing tools  that might help your model.   Look at the  model development page  to create and compare models.  Look at the  deployment page  to deploy model.",
            "title": "If you want create a model on your data via machine learning"
        },
        {
            "location": "/getting-started/where-do-i-begin/#if-you-want-to-do-general-healthcare-statistics",
            "text": "Look  here  to calculate cross-column correlations on your data.  Look  here  to perform trend analysis across your data following Nelson rules.",
            "title": "If you want to do general healthcare statistics"
        },
        {
            "location": "/getting-started/where-do-i-begin/#if-you-want-to-do-risk-adjusted-comparisons",
            "text": "Look  here  if you want to compare several hospitals or units in terms of risk-adjusted performance.",
            "title": "If you want to do risk-adjusted comparisons"
        },
        {
            "location": "/getting-started/FAQ/",
            "text": "HCRTools FAQ - Frequently Asked Questions\n\n\nWho is this project for?\n\n\nWhile data scientists in healthcare will likely find this project valuable, the audience HCTools targets are those BI developers, data architects, and SQL-based folks that would love to create appropriate and accurate models on their data. While packages like scikit-learn are certainly irreplaceable, we think that there is a set of data problems specific to healthcare that warrant new tools.\n\n\nHow does HCRTools focus on healthcare?\n\n\nHCTools differs from other machine learning packages in that it focuses on data issues specific to healthcare. This means that we pay attention to longitudinal questions, offer easy solutions specific to data with imbalanced classes, and provide easy connections and deployment to SQL Server.\n\n\nWho started this project?\n\n\nThis project began in the data science group at Health Catalyst, a Salt Lake City-based company based focused on improving healthcare outcomes.\n\n\nWhy was it open-sourced?\n\n\nWe believe that everyone benefits when healthcare is made more efficient and outcomes are improved. Machine learning is surprisingly new to healthcare and we want to quickly take healthcare down the machine learning adoption path. We believe that making helpful, simple tools widely available is one small way to make US healthcare a little less kludgy.",
            "title": "FAQ"
        },
        {
            "location": "/getting-started/FAQ/#hcrtools-faq-frequently-asked-questions",
            "text": "",
            "title": "HCRTools FAQ - Frequently Asked Questions"
        },
        {
            "location": "/getting-started/FAQ/#who-is-this-project-for",
            "text": "While data scientists in healthcare will likely find this project valuable, the audience HCTools targets are those BI developers, data architects, and SQL-based folks that would love to create appropriate and accurate models on their data. While packages like scikit-learn are certainly irreplaceable, we think that there is a set of data problems specific to healthcare that warrant new tools.",
            "title": "Who is this project for?"
        },
        {
            "location": "/getting-started/FAQ/#how-does-hcrtools-focus-on-healthcare",
            "text": "HCTools differs from other machine learning packages in that it focuses on data issues specific to healthcare. This means that we pay attention to longitudinal questions, offer easy solutions specific to data with imbalanced classes, and provide easy connections and deployment to SQL Server.",
            "title": "How does HCRTools focus on healthcare?"
        },
        {
            "location": "/getting-started/FAQ/#who-started-this-project",
            "text": "This project began in the data science group at Health Catalyst, a Salt Lake City-based company based focused on improving healthcare outcomes.",
            "title": "Who started this project?"
        },
        {
            "location": "/getting-started/FAQ/#why-was-it-open-sourced",
            "text": "We believe that everyone benefits when healthcare is made more efficient and outcomes are improved. Machine learning is surprisingly new to healthcare and we want to quickly take healthcare down the machine learning adoption path. We believe that making helpful, simple tools widely available is one small way to make US healthcare a little less kludgy.",
            "title": "Why was it open-sourced?"
        },
        {
            "location": "/model-pre-processing/feature-eng-overview/",
            "text": "Overview of pre-processing and feature engineering\n\n\nWhat is feature engineering?\n\n\nFrom wikipedia: \"Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\"\n\n\nIn other words, while you might have great data, it has to be in a form that the algorithms can use to make a model.\n\n\nHow can HCRTools help me prepare data prior to model-creation?\n\n\n\n\n\n\nVia \nlongitudinal imputation\n. Think of this as a way to pull person-specific values forward in time.\n\n\n\n\n\n\nVia \nseasonality handling\n. Think of this as a way to make a date-time column model-ready.",
            "title": "Feature engineering overview"
        },
        {
            "location": "/model-pre-processing/feature-eng-overview/#overview-of-pre-processing-and-feature-engineering",
            "text": "",
            "title": "Overview of pre-processing and feature engineering"
        },
        {
            "location": "/model-pre-processing/feature-eng-overview/#what-is-feature-engineering",
            "text": "From wikipedia: \"Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\"  In other words, while you might have great data, it has to be in a form that the algorithms can use to make a model.",
            "title": "What is feature engineering?"
        },
        {
            "location": "/model-pre-processing/feature-eng-overview/#how-can-hcrtools-help-me-prepare-data-prior-to-model-creation",
            "text": "Via  longitudinal imputation . Think of this as a way to pull person-specific values forward in time.    Via  seasonality handling . Think of this as a way to make a date-time column model-ready.",
            "title": "How can HCRTools help me prepare data prior to model-creation?"
        },
        {
            "location": "/model-pre-processing/longitudinal-imputation/",
            "text": "Longitudinal Imputation via \nGroupedLOCF\n\n\nWhat is this?\n\n\nIn healthcare one often works with datasets that have multiple rows for a single person, over time. This is called longitudinal data.\n\n\nIf you want to fill in some of the NULLs in such a dataset, HCRTools lets you pull values forward within each person's particular history. In other words, Joe's weight from a year ago can be pulled forward to Joe's rows corresponding to last week or last month.\n\n\nWhy is it helpful?\n\n\nThis may help make your models more accurate, or help fill in your data for disparate calculations/visualizations.\n\n\nIs any longitudinal dataset ready for HCRTools to work on it?\n\n\nNope. You have to first order your data by a PersonID column and then by a date-time column (with time going down the rows).\n\n\nSo, how do we do it?\n\n\n\n\nFirst, we'll load HCRTools and create a fake dataset in R (that you can play with)\n\n\n\n\nlibrary(HCRTools)\ndf = data.frame(PersonID=c(1,1,2,2,3,3,3),\n                wt=c(.5,NA,NA,NA,.3,.7,NA),\n                ht=c(NA,1,3,NA,4,NA,NA),\n                date=c('01/01/2015','01/15/2015','01/01/2015','01/15/2015',\n                       '01/01/2015','01/15/2015','01/30/2015'))\n\nhead(df,n=7) # Looking at the raw data\n\n\n\n\n\n\nNow let's do the imputation by calling the \nGroupedLOCF\n function. LOCF stands for last value carried forward\n\n\n\n\ndf.result = GroupedLOCF(df, 'PersonID')\n\nhead(df.result,n=7) # Looking at the data that now has fewer NULLs (or NAs)\n\n\n\n\nFunction specs for \nGroupedLOCF\n\n\n\n\n\n\nReturn\n: a data frame of same shape as input data frame.\n\n\n\n\n\n\nArguments\n:\n\n\n\n\ndf\n: a data frame. This data contains NULLs or NAs.\n\n\nid\n: a string. Column name for the PersonID column in your data frame.\n\n\n\n\n\n\n\n\nFull example code\n\n\nlibrary(HCRTools)\ndf = data.frame(PersonID=c(1,1,2,2,3,3,3),\n                wt=c(.5,NA,NA,NA,.3,.7,NA),\n                ht=c(NA,1,3,NA,4,NA,NA),\n                date=c('01/01/2015','01/15/2015','01/01/2015','01/15/2015',\n                       '01/01/2015','01/15/2015','01/30/2015'))\n\nhead(df,n=7) # Looking at the raw data\n\ndf.result = GroupedLOCF(df, 'PersonID')\n\nhead(df.result,n=7) # Looking at the data that now has fewer NULLs (or NAs)",
            "title": "Longitudinal Imputation"
        },
        {
            "location": "/model-pre-processing/longitudinal-imputation/#longitudinal-imputation-via-groupedlocf",
            "text": "",
            "title": "Longitudinal Imputation via GroupedLOCF"
        },
        {
            "location": "/model-pre-processing/longitudinal-imputation/#what-is-this",
            "text": "In healthcare one often works with datasets that have multiple rows for a single person, over time. This is called longitudinal data.  If you want to fill in some of the NULLs in such a dataset, HCRTools lets you pull values forward within each person's particular history. In other words, Joe's weight from a year ago can be pulled forward to Joe's rows corresponding to last week or last month.",
            "title": "What is this?"
        },
        {
            "location": "/model-pre-processing/longitudinal-imputation/#why-is-it-helpful",
            "text": "This may help make your models more accurate, or help fill in your data for disparate calculations/visualizations.",
            "title": "Why is it helpful?"
        },
        {
            "location": "/model-pre-processing/longitudinal-imputation/#is-any-longitudinal-dataset-ready-for-hcrtools-to-work-on-it",
            "text": "Nope. You have to first order your data by a PersonID column and then by a date-time column (with time going down the rows).",
            "title": "Is any longitudinal dataset ready for HCRTools to work on it?"
        },
        {
            "location": "/model-pre-processing/longitudinal-imputation/#so-how-do-we-do-it",
            "text": "First, we'll load HCRTools and create a fake dataset in R (that you can play with)   library(HCRTools)\ndf = data.frame(PersonID=c(1,1,2,2,3,3,3),\n                wt=c(.5,NA,NA,NA,.3,.7,NA),\n                ht=c(NA,1,3,NA,4,NA,NA),\n                date=c('01/01/2015','01/15/2015','01/01/2015','01/15/2015',\n                       '01/01/2015','01/15/2015','01/30/2015'))\n\nhead(df,n=7) # Looking at the raw data   Now let's do the imputation by calling the  GroupedLOCF  function. LOCF stands for last value carried forward   df.result = GroupedLOCF(df, 'PersonID')\n\nhead(df.result,n=7) # Looking at the data that now has fewer NULLs (or NAs)",
            "title": "So, how do we do it?"
        },
        {
            "location": "/model-pre-processing/longitudinal-imputation/#function-specs-for-groupedlocf",
            "text": "Return : a data frame of same shape as input data frame.    Arguments :   df : a data frame. This data contains NULLs or NAs.  id : a string. Column name for the PersonID column in your data frame.",
            "title": "Function specs for GroupedLOCF"
        },
        {
            "location": "/model-pre-processing/longitudinal-imputation/#full-example-code",
            "text": "library(HCRTools)\ndf = data.frame(PersonID=c(1,1,2,2,3,3,3),\n                wt=c(.5,NA,NA,NA,.3,.7,NA),\n                ht=c(NA,1,3,NA,4,NA,NA),\n                date=c('01/01/2015','01/15/2015','01/01/2015','01/15/2015',\n                       '01/01/2015','01/15/2015','01/30/2015'))\n\nhead(df,n=7) # Looking at the raw data\n\ndf.result = GroupedLOCF(df, 'PersonID')\n\nhead(df.result,n=7) # Looking at the data that now has fewer NULLs (or NAs)",
            "title": "Full example code"
        },
        {
            "location": "/model-pre-processing/seasonality-handling/",
            "text": "Seasonality handling via \nConvertDateTimeColToDummies\n\n\nWhat is this?\n\n\nIn healthcare date-time stamp columns are common, but unfortunately machine learning algorithms do not handle them well.\n\n\nWhy is it helpful?\n\n\nOne has to do some simple feature engineering in order to take advantage of potential seasonality effects in a dataset. This works simply by transforming a date-time column into multiple columns that represent MonthOfYear, WeekOfYear, DayOfMonth, DayOfWeek, etc.\n\n\nSo, how do we do it?\n\n\n\n\nFirst, we'll load HCRTools, create a fake dataset on which to work, and look at it:\n\n\n\n\nDTCol = c(\n2001-06-09 12:45:05\n,\n2002-01-29 09:30:05\n,\n2002-02-02 07:36:50\n,\n          \n2002-03-04 16:45:01\n,\n2002-11-13 20:00:10\n,\n2003-01-29 07:31:43\n,\n          \n2003-07-07 17:30:02\n,\n2003-09-28 01:03:20\n)\ny1 \n- c(.5,1,3,6,8,13,14,1)\ny2 \n- c(.8,1,1.2,1.2,1.2,1.3,1.3,1)\ndf \n- data.frame(DTCol,y1,y2)\n\nhead(df)\n\n\n\n\n\n\nNext, we'll create the extra date and time colums by calling the function and then we'll look at the transformed dataset\n\n\n\n\ndf.result \n- ConvertDateTimeColToDummies(df, 'DTCol')\n\nhead(df.result)\n\n\n\n\nFunction specs for \nConvertDateTimeColToDummies\n\n\n\n\n\n\nReturn\n: a data frame of same length, but greater width compared to the input data frame.\n\n\n\n\n\n\nArguments\n:\n\n\n\n\ndf\n: a data frame. This data frame contains a date-time column.\n\n\ndate.time.col\n: a string. Column name for the date-time column in your data frame that you want to split into multiple date and time columns. Works best in ISO 8601 format (ie, datetime or datetime2 in T-SQL).\n\n\ndepth\n: a string, defaults to 'h'. Indicates how many columns should be added to data frame. 'd', 'h', 'm', 's' expands to depth of day, hour, minute, and second, respectively. \n\n\nreturn.dt.col\n: boolean, defaults to FALSE. Indicates whether to return original date-time column with modified data frame.\n\n\n\n\n\n\n\n\nFull example code\n\n\nlibrary(HCRTools)\nDTCol = c(\n2001-06-09 12:45:05\n,\n2002-01-29 09:30:05\n,\n2002-02-02 07:36:50\n,\n          \n2002-03-04 16:45:01\n,\n2002-11-13 20:00:10\n,\n2003-01-29 07:31:43\n,\n          \n2003-07-07 17:30:02\n,\n2003-09-28 01:03:20\n)\ny1 \n- c(.5,1,3,6,8,13,14,1)\ny2 \n- c(.8,1,1.2,1.2,1.2,1.3,1.3,1)\ndf \n- data.frame(DTCol,y1,y2)\n\ndf \n- ConvertDateTimeColToDummies(df, 'DTCol')",
            "title": "Seasonality Handling"
        },
        {
            "location": "/model-pre-processing/seasonality-handling/#seasonality-handling-via-convertdatetimecoltodummies",
            "text": "",
            "title": "Seasonality handling via ConvertDateTimeColToDummies"
        },
        {
            "location": "/model-pre-processing/seasonality-handling/#what-is-this",
            "text": "In healthcare date-time stamp columns are common, but unfortunately machine learning algorithms do not handle them well.",
            "title": "What is this?"
        },
        {
            "location": "/model-pre-processing/seasonality-handling/#why-is-it-helpful",
            "text": "One has to do some simple feature engineering in order to take advantage of potential seasonality effects in a dataset. This works simply by transforming a date-time column into multiple columns that represent MonthOfYear, WeekOfYear, DayOfMonth, DayOfWeek, etc.",
            "title": "Why is it helpful?"
        },
        {
            "location": "/model-pre-processing/seasonality-handling/#so-how-do-we-do-it",
            "text": "First, we'll load HCRTools, create a fake dataset on which to work, and look at it:   DTCol = c( 2001-06-09 12:45:05 , 2002-01-29 09:30:05 , 2002-02-02 07:36:50 ,\n           2002-03-04 16:45:01 , 2002-11-13 20:00:10 , 2003-01-29 07:31:43 ,\n           2003-07-07 17:30:02 , 2003-09-28 01:03:20 )\ny1  - c(.5,1,3,6,8,13,14,1)\ny2  - c(.8,1,1.2,1.2,1.2,1.3,1.3,1)\ndf  - data.frame(DTCol,y1,y2)\n\nhead(df)   Next, we'll create the extra date and time colums by calling the function and then we'll look at the transformed dataset   df.result  - ConvertDateTimeColToDummies(df, 'DTCol')\n\nhead(df.result)",
            "title": "So, how do we do it?"
        },
        {
            "location": "/model-pre-processing/seasonality-handling/#function-specs-for-convertdatetimecoltodummies",
            "text": "Return : a data frame of same length, but greater width compared to the input data frame.    Arguments :   df : a data frame. This data frame contains a date-time column.  date.time.col : a string. Column name for the date-time column in your data frame that you want to split into multiple date and time columns. Works best in ISO 8601 format (ie, datetime or datetime2 in T-SQL).  depth : a string, defaults to 'h'. Indicates how many columns should be added to data frame. 'd', 'h', 'm', 's' expands to depth of day, hour, minute, and second, respectively.   return.dt.col : boolean, defaults to FALSE. Indicates whether to return original date-time column with modified data frame.",
            "title": "Function specs for ConvertDateTimeColToDummies"
        },
        {
            "location": "/model-pre-processing/seasonality-handling/#full-example-code",
            "text": "library(HCRTools)\nDTCol = c( 2001-06-09 12:45:05 , 2002-01-29 09:30:05 , 2002-02-02 07:36:50 ,\n           2002-03-04 16:45:01 , 2002-11-13 20:00:10 , 2003-01-29 07:31:43 ,\n           2003-07-07 17:30:02 , 2003-09-28 01:03:20 )\ny1  - c(.5,1,3,6,8,13,14,1)\ny2  - c(.8,1,1.2,1.2,1.2,1.3,1.3,1)\ndf  - data.frame(DTCol,y1,y2)\n\ndf  - ConvertDateTimeColToDummies(df, 'DTCol')",
            "title": "Full example code"
        },
        {
            "location": "/comparing-and-deploying/compare/",
            "text": "Create and compare models via \nLasso\n and \nRandomForest\n\n\nWhat is this?\n\n\nThese classes let one create and compare custom models on varied datasets.\n\n\nOne can do both classification (ie, predict Y or N) as well as regression (ie, predict a numeric field).\n\n\nIs any dataset ready for model creation?\n\n\nNope. It'll help if you can follow these guidelines:\n\n\n\n\nDon't use 0 or 1 for the independent variable when doing classification. Use Y/N instead. The IIF function in T-SQL may help here.\n\n\nDon't pull in test data in this step. In other words, to compare models, we don't need to worry about those rows that need a prediction quite yet.\n\n\n\n\nHow can I improve my model performance?\n\n\n\n\nIf you have lots of NULL cells and your data is longitudinal, you may want to try \nGroupedLOCF\n.\n\n\nIf you think the phenomenon you're trying to predict has a seasonal or diurnal component, you may need some \nfeature engineering\n.\n\n\n\n\nStep 1: Pull in the data via \nSelectData\n\n\n\n\n\n\nReturn\n: a data frame that represents your data.\n\n\n\n\n\n\nArguments\n:\n\n\n\n\nserver\n: a server name. You'll pull data from this server.\n\n\ndatabase\n: a database name. You'll pull data from this database.\n\n\n\n\n\n\n\n\nptm \n- proc.time()\nlibrary(HCRTools)\n\nconnection.string = \n\ndriver={SQL Server};\nserver=localhost;\ndatabase=AdventureWorks2012;\ntrusted_connection=true\n\n\n\nquery = \n\nSELECT\n[OrganizationLevel]\n,[MaritalStatus]\n,[Gender]\n,IIF([SalariedFlag]=0,'N','Y') AS SalariedFlag\n,[VacationHours]\n,[SickLeaveHours]\nFROM [AdventureWorks2012].[HumanResources].[Employee]\n\n\n\ndf \n- SelectData(connection.string, query)\nhead(df)\nstr(df)\n\n\n\n\nStep 2: Set your parameters via \nSupervisedModelParameters\n\n\n\n\n\n\nReturn\n: an object representing your specific configuration.\n\n\n\n\n\n\nArguments\n:\n\n\n\n\ndf\n: a data frame. The data your model is based on.\n\n\ntype\n: a string. This will either be 'CLASSIFICATION' or 'REGRESSION'.\n\n\nimpute\n: a boolean, defaults to FALSE. Whether to impute by replacing NULLs with column mean (for numeric columns) or column mode (for categorical columns).\n\n\ngrainCol\n: a string, defaults to None. Name of possible GrainID column in your dataset. If specified, this column will be removed, as it won't help the algorithm.\n\n\npredictedCol\n: a string. Name of variable (or column) that you want to predict. \n\n\ndebug\n: a boolean, defaults to FALSE. If TRUE, console output when comparing models is verbose for easier debugging.\n\n\ncores\n: an int, defaults to 4. Number of cores on machine to use for model training.\n\n\n\n\n\n\n\n\np \n- SupervisedModelParameters$new()\np$df = df\np$type = 'CLASSIFICATION'\np$impute = TRUE\np$grainCol = ''\np$predictedCol = 'SalariedFlag'\np$debug = FALSE\np$cores = 1\n\n\n\n\nStep 3: Create the models via the \nLasso\n and \nRandomForest\n algorithms.\n\n\n# Run Lasso\nlasso \n- Lasso$new(p)\nlasso$run()\n\n# Run RandomForest\nrf \n- RandomForest$new(p)\nrf$run()\n\n\n\n\nFull example code",
            "title": "Develop and Compare"
        },
        {
            "location": "/comparing-and-deploying/compare/#create-and-compare-models-via-lasso-and-randomforest",
            "text": "",
            "title": "Create and compare models via Lasso and RandomForest"
        },
        {
            "location": "/comparing-and-deploying/compare/#what-is-this",
            "text": "These classes let one create and compare custom models on varied datasets.  One can do both classification (ie, predict Y or N) as well as regression (ie, predict a numeric field).",
            "title": "What is this?"
        },
        {
            "location": "/comparing-and-deploying/compare/#is-any-dataset-ready-for-model-creation",
            "text": "Nope. It'll help if you can follow these guidelines:   Don't use 0 or 1 for the independent variable when doing classification. Use Y/N instead. The IIF function in T-SQL may help here.  Don't pull in test data in this step. In other words, to compare models, we don't need to worry about those rows that need a prediction quite yet.",
            "title": "Is any dataset ready for model creation?"
        },
        {
            "location": "/comparing-and-deploying/compare/#how-can-i-improve-my-model-performance",
            "text": "If you have lots of NULL cells and your data is longitudinal, you may want to try  GroupedLOCF .  If you think the phenomenon you're trying to predict has a seasonal or diurnal component, you may need some  feature engineering .",
            "title": "How can I improve my model performance?"
        },
        {
            "location": "/comparing-and-deploying/compare/#step-1-pull-in-the-data-via-selectdata",
            "text": "Return : a data frame that represents your data.    Arguments :   server : a server name. You'll pull data from this server.  database : a database name. You'll pull data from this database.     ptm  - proc.time()\nlibrary(HCRTools)\n\nconnection.string =  \ndriver={SQL Server};\nserver=localhost;\ndatabase=AdventureWorks2012;\ntrusted_connection=true \n\nquery =  \nSELECT\n[OrganizationLevel]\n,[MaritalStatus]\n,[Gender]\n,IIF([SalariedFlag]=0,'N','Y') AS SalariedFlag\n,[VacationHours]\n,[SickLeaveHours]\nFROM [AdventureWorks2012].[HumanResources].[Employee] \n\ndf  - SelectData(connection.string, query)\nhead(df)\nstr(df)",
            "title": "Step 1: Pull in the data via SelectData"
        },
        {
            "location": "/comparing-and-deploying/compare/#step-2-set-your-parameters-via-supervisedmodelparameters",
            "text": "Return : an object representing your specific configuration.    Arguments :   df : a data frame. The data your model is based on.  type : a string. This will either be 'CLASSIFICATION' or 'REGRESSION'.  impute : a boolean, defaults to FALSE. Whether to impute by replacing NULLs with column mean (for numeric columns) or column mode (for categorical columns).  grainCol : a string, defaults to None. Name of possible GrainID column in your dataset. If specified, this column will be removed, as it won't help the algorithm.  predictedCol : a string. Name of variable (or column) that you want to predict.   debug : a boolean, defaults to FALSE. If TRUE, console output when comparing models is verbose for easier debugging.  cores : an int, defaults to 4. Number of cores on machine to use for model training.     p  - SupervisedModelParameters$new()\np$df = df\np$type = 'CLASSIFICATION'\np$impute = TRUE\np$grainCol = ''\np$predictedCol = 'SalariedFlag'\np$debug = FALSE\np$cores = 1",
            "title": "Step 2: Set your parameters via SupervisedModelParameters"
        },
        {
            "location": "/comparing-and-deploying/compare/#step-3-create-the-models-via-the-lasso-and-randomforest-algorithms",
            "text": "# Run Lasso\nlasso  - Lasso$new(p)\nlasso$run()\n\n# Run RandomForest\nrf  - RandomForest$new(p)\nrf$run()",
            "title": "Step 3: Create the models via the Lasso and RandomForest algorithms."
        },
        {
            "location": "/comparing-and-deploying/compare/#full-example-code",
            "text": "",
            "title": "Full example code"
        },
        {
            "location": "/comparing-and-deploying/deploy/",
            "text": "deploy",
            "title": "Deploying a Best Model"
        },
        {
            "location": "/comparing-and-deploying/deploy/#deploy",
            "text": "",
            "title": "deploy"
        },
        {
            "location": "/healthcare-statistics/risk-adjusted-comparisons/",
            "text": "risk-adjusted-comparison",
            "title": "Risk-adjusted Comparisons"
        },
        {
            "location": "/healthcare-statistics/risk-adjusted-comparisons/#risk-adjusted-comparison",
            "text": "",
            "title": "risk-adjusted-comparison"
        },
        {
            "location": "/healthcare-statistics/trend-analysis/",
            "text": "trend-analysis",
            "title": "Trend Analysis"
        },
        {
            "location": "/healthcare-statistics/trend-analysis/#trend-analysis",
            "text": "",
            "title": "trend-analysis"
        },
        {
            "location": "/healthcare-statistics/find-correlations/",
            "text": "find-correlations",
            "title": "Find Correlations"
        },
        {
            "location": "/healthcare-statistics/find-correlations/#find-correlations",
            "text": "",
            "title": "find-correlations"
        }
    ]
}